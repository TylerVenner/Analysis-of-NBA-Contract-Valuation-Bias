{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de1f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Listing Columns for Each Dataset ---\n",
      "--- Looking in directory: c:\\Users\\tyler\\School\\Learn Statistics\\STA 160\\Project\\data\\raw ---\n",
      "\n",
      "ðŸ“„ File: ../../data/raw/Player_Performance_raw.csv\n",
      "Columns:\n",
      "  - PLAYER_ID\n",
      "  - PLAYER_NAME\n",
      "  - TEAM_ID\n",
      "  - E_OFF_RATING\n",
      "  - OFF_RATING\n",
      "  - sp_work_OFF_RATING\n",
      "  - E_DEF_RATING\n",
      "  - DEF_RATING\n",
      "  - sp_work_DEF_RATING\n",
      "  - E_NET_RATING\n",
      "  - NET_RATING\n",
      "  - sp_work_NET_RATING\n",
      "  - AST_PCT\n",
      "  - AST_TO\n",
      "  - AST_RATIO\n",
      "  - OREB_PCT\n",
      "  - DREB_PCT\n",
      "  - REB_PCT\n",
      "  - TM_TOV_PCT\n",
      "  - E_TOV_PCT\n",
      "  - EFG_PCT\n",
      "  - TS_PCT\n",
      "  - USG_PCT\n",
      "  - E_USG_PCT\n",
      "  - E_PACE\n",
      "  - PACE\n",
      "  - PACE_PER40\n",
      "  - sp_work_PACE\n",
      "  - PIE\n",
      "  - POSS\n",
      "  - FGM_PG\n",
      "  - FGA_PG\n",
      "  - E_OFF_RATING_RANK\n",
      "  - OFF_RATING_RANK\n",
      "  - sp_work_OFF_RATING_RANK\n",
      "  - E_DEF_RATING_RANK\n",
      "  - DEF_RATING_RANK\n",
      "  - sp_work_DEF_RATING_RANK\n",
      "  - E_NET_RATING_RANK\n",
      "  - NET_RATING_RANK\n",
      "  - sp_work_NET_RATING_RANK\n",
      "  - AST_PCT_RANK\n",
      "  - AST_TO_RANK\n",
      "  - AST_RATIO_RANK\n",
      "  - OREB_PCT_RANK\n",
      "  - DREB_PCT_RANK\n",
      "  - REB_PCT_RANK\n",
      "  - TM_TOV_PCT_RANK\n",
      "  - E_TOV_PCT_RANK\n",
      "  - EFG_PCT_RANK\n",
      "  - TS_PCT_RANK\n",
      "  - USG_PCT_RANK\n",
      "  - E_USG_PCT_RANK\n",
      "  - E_PACE_RANK\n",
      "  - PACE_RANK\n",
      "  - sp_work_PACE_RANK\n",
      "  - PIE_RANK\n",
      "  - FGM_PG_RANK\n",
      "  - FGA_PG_RANK\n",
      "\n",
      "ðŸ“„ File: ../../data/raw/raw_player_context.csv\n",
      "Columns:\n",
      "  - PLAYER_ID\n",
      "  - PLAYER_NAME\n",
      "  - BIRTHDATE\n",
      "  - COUNTRY\n",
      "  - DRAFT_YEAR\n",
      "  - DRAFT_ROUND\n",
      "  - DRAFT_NUMBER\n",
      "\n",
      "ðŸ“„ File: ../../data/raw/raw_player_salaries.csv\n",
      "Columns:\n",
      "  - Player_Name\n",
      "  - Salary\n",
      "\n",
      "ðŸ“„ File: ../../data/raw/raw_salary_caps.csv\n",
      "Columns:\n",
      "  - team\n",
      "  - team_id\n",
      "  - record\n",
      "  - active_players\n",
      "  - avg_team_age\n",
      "  - total_cap_used\n",
      "  - remaining_cap_space\n",
      "  - active_cap\n",
      "  - active_top_3\n",
      "  - dead_cap\n",
      "\n",
      "ðŸ“„ File: ../../data/raw/nba_player_popularity.csv\n",
      "Columns:\n",
      "  - Player\n",
      "  - Followers\n",
      "\n",
      "ðŸ“„ File: ../../data/raw/nba_stadiums.csv\n",
      "Columns:\n",
      "  - TEAM_ABBREVIATION\n",
      "  - Stadium_Name\n",
      "  - Capacity\n",
      "  - City\n",
      "  - Year_Opened\n",
      "  - Construction_Cost\n",
      "\n",
      "ðŸ“„ File: ../../data/raw/Owner Net Worth in Billions .csv\n",
      "Columns:\n",
      "  - Team Name\n",
      "  - Owner Net Worth in Billions\n",
      "  - Team ID\n",
      "  - Unnamed: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os # Import the os module to handle paths correctly\n",
    "\n",
    "def list_all_csv_columns():\n",
    "    \"\"\"\n",
    "    Reads all CSV files in the current directory and prints their column names.\n",
    "    \"\"\"\n",
    "    # Get a list of all CSV files provided by the user\n",
    "    # We'll hardcode the list based on the uploaded files\n",
    "    # to ensure we only read the ones relevant to this task.\n",
    "    \n",
    "    # Base directory where the files are located\n",
    "    base_path = \"../../data/raw/\"\n",
    "    \n",
    "    original_filenames = [\n",
    "        \"Player_Performance_raw.csv\",\n",
    "        \"raw_player_context.csv\",\n",
    "        \"raw_player_salaries.csv\",\n",
    "        \"raw_salary_caps.csv\",\n",
    "        \"nba_player_popularity.csv\",\n",
    "        \"nba_stadiums.csv\",\n",
    "        \"Owner Net Worth in Billions .csv\"\n",
    "    ]\n",
    "    \n",
    "    # Create the full file paths by joining the base path and the filename\n",
    "    filenames = [os.path.join(base_path, f) for f in original_filenames]\n",
    "\n",
    "    print(\"--- Listing Columns for Each Dataset ---\")\n",
    "    print(f\"--- Looking in directory: {os.path.abspath(base_path)} ---\") # Added to show the full path it's trying\n",
    "\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            # Use skipinitialspace=True to handle spaces after delimiters,\n",
    "            # which can mess up column names.\n",
    "            df = pd.read_csv(filename, skipinitialspace=True)\n",
    "            \n",
    "            # Clean up column names by stripping leading/trailing whitespace\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            print(f\"\\nðŸ“„ File: {filename}\") # This will now print the full path\n",
    "            print(\"Columns:\")\n",
    "            for col in df.columns:\n",
    "                print(f\"  - {col}\")\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"\\nâŒ ERROR: File not found: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ ERROR: Could not read {filename}. Reason: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    list_all_csv_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a42d2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Player Merge Process ---\n",
      "\n",
      "--- [Step 1] Loading Player Data ---\n",
      "Attempting to load file: '../../data/raw/Player_Performance_raw.csv'...\n",
      "âœ… Successfully loaded '../../data/raw/Player_Performance_raw.csv'. Found 569 rows.\n",
      "Attempting to load file: '../../data/raw/raw_player_context.csv'...\n",
      "âœ… Successfully loaded '../../data/raw/raw_player_context.csv'. Found 569 rows.\n",
      "Attempting to load file: '../../data/raw/raw_player_salaries.csv'...\n",
      "âœ… Successfully loaded '../../data/raw/raw_player_salaries.csv'. Found 450 rows.\n",
      "Attempting to load file: '../../data/raw/nba_player_popularity.csv'...\n",
      "âœ… Successfully loaded '../../data/raw/nba_player_popularity.csv'. Found 511 rows.\n",
      "\n",
      "--- [Step 2] Merging Stats and Context (on PLAYER_ID) ---\n",
      "Stats rows: 569 | Context rows: 569\n",
      "Merge complete. Result rows: 569\n",
      "\n",
      "--- [Step 3] Merging Salaries (on Standardized Name) ---\n",
      "\n",
      "--- DEBUG: Checking standardized names for merge ---\n",
      "\n",
      "--- Names from Player Base (Stats/Context): ---\n",
      "   PLAYER_NAME_context          merge_key\n",
      "0         LeBron James       lebron james\n",
      "1           Chris Paul         chris paul\n",
      "2           Kyle Lowry         kyle lowry\n",
      "3          P.J. Tucker          pj tucker\n",
      "4         Kevin Durant       kevin durant\n",
      "5           Al Horford         al horford\n",
      "6          Mike Conley        mike conley\n",
      "7           Jeff Green         jeff green\n",
      "8    Russell Westbrook  russell westbrook\n",
      "9           Kevin Love         kevin love\n",
      "10         Eric Gordon        eric gordon\n",
      "11         Brook Lopez        brook lopez\n",
      "12       Nicolas Batum      nicolas batum\n",
      "13      DeAndre Jordan     deandre jordan\n",
      "14        James Harden       james harden\n",
      "\n",
      "--- Names from Salary File: ---\n",
      "                     Player_Name           merge_key\n",
      "0              Young  Trae Young          trae young\n",
      "1           Capela  Clint Capela        clint capela\n",
      "2           LeVert  Caris LeVert        caris levert\n",
      "3        Okongwu  Onyeka Okongwu      onyeka okongwu\n",
      "4   Risacher  Zaccharie Risacher  zaccharie risacher\n",
      "5             Mann  Terance Mann        terance mann\n",
      "6     Nance Jr.  Larry Nance Jr.   jr larry nance jr\n",
      "7           Niang  Georges Niang       georges niang\n",
      "8         Daniels  Dyson Daniels       dyson daniels\n",
      "9         Johnson  Jalen Johnson       jalen johnson\n",
      "10           Bufkin  Kobe Bufkin         kobe bufkin\n",
      "11     Mathews  Garrison Mathews    garrison mathews\n",
      "12            Krejci  Vit Krejci          vit krejci\n",
      "13         Gueye  Mouhamed Gueye      mouhamed gueye\n",
      "14       Barlow  Dominick Barlow     dominick barlow\n",
      "--- End Debug ---\n",
      "Player Base rows: 569 | Salary rows: 450\n",
      "Merge complete. Result rows: 409\n",
      "âš ï¸ Players dropped (no salary match): 160\n",
      "\n",
      "--- [Step 4] Merging Player Popularity (on Standardized Name) ---\n",
      "Player Master rows: 409 | Popularity rows: 511\n",
      "Left merge complete. Result rows: 409\n",
      "Players kept (from left merge): 409\n",
      "\n",
      "--- [Step 5] Cleaning and Saving Player Dataset ---\n",
      "\n",
      "âœ… --- PLAYER MERGE COMPLETE --- âœ…\n",
      "Final player dataset with 409 rows saved to:\n",
      "C:\\Users\\tyler\\School\\Learn Statistics\\STA 160\\Project\\notebooks\\Tyler\\merged_player_data.csv\n",
      "\n",
      "--- NEXT STEP ---\n",
      "We now need to merge the team data (Caps, Owners, Stadiums).\n",
      "This will require creating a mapping between 'TEAM_ID', 'team' (abbreviation), and 'Team Name'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# --- File Configuration ---\n",
    "BASE_PATH = '../../data/raw/' # Set the base path for all files\n",
    "Path(BASE_PATH).mkdir(parents=True, exist_ok=True) # Ensure data/raw directory exists just in case\n",
    "\n",
    "# Source files (Originals)\n",
    "STATS_FILE = os.path.join(BASE_PATH, 'Player_Performance_raw.csv')\n",
    "CONTEXT_FILE = os.path.join(BASE_PATH, 'raw_player_context.csv')\n",
    "SALARY_FILE = os.path.join(BASE_PATH, 'raw_player_salaries.csv')\n",
    "POPULARITY_FILE = os.path.join(BASE_PATH, 'nba_player_popularity.csv')\n",
    "\n",
    "# --- NOTE: Team files will be handled in a separate step ---\n",
    "# CAPS_FILE = os.path.join(BASE_PATH, 'raw_salary_caps.csv')\n",
    "# OWNERS_FILE = os.path.join(BASE_PATH, 'Owner Net Worth in Billions .csv')\n",
    "# STADIUMS_FILE = os.path.join(BASE_PATH, 'nba_stadiums.csv')\n",
    "\n",
    "# Final output file\n",
    "OUTPUT_FILE = 'merged_player_data.csv' # Output for this script\n",
    "\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "def standardize_player_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans and standardizes player names so merges work across data sources.\n",
    "    (Copied from your cleaning_helpers.py)\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize unicode (e.g., accents)\n",
    "    try:\n",
    "        # Handle potential empty strings or NaNs that become float\n",
    "        name = str(name)\n",
    "        name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not normalize name '{name}'. Error: {e}\")\n",
    "        pass # Continue with the original name if normalization fails\n",
    "\n",
    "    # Replace punctuation and collapse spaces\n",
    "    name = re.sub(r'[^a-zA-Z\\s]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    \n",
    "    return name.strip().lower()\n",
    "\n",
    "\n",
    "def load_data(filename: str, required_cols: list = None, dtype: dict = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a CSV file with robust debugging and error checking.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to load file: '{filename}'...\")\n",
    "    \n",
    "    # 1. Check if file exists\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"--- ðŸ”´ ERROR: File not found! ---\")\n",
    "        print(f\"Script stopped. Could not find file: {filename}\")\n",
    "        sys.exit(1) # Stop the script\n",
    "        \n",
    "    # 2. Load the data\n",
    "    try:\n",
    "        # Use skipinitialspace=True to handle spaces in column names from output\n",
    "        df = pd.read_csv(filename, skipinitialspace=True, dtype=dtype)\n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        print(f\"âœ… Successfully loaded '{filename}'. Found {len(df)} rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"--- ðŸ”´ ERROR: Could not read file! ---\")\n",
    "        print(f\"Could not load {filename}. Error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 3. Check for required columns\n",
    "    if required_cols:\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"--- ðŸ”´ ERROR: Missing required columns in '{filename}'! ---\")\n",
    "            print(f\"Expected columns: {required_cols}\")\n",
    "            print(f\"Missing columns: {missing_cols}\")\n",
    "            print(f\"All columns found: {list(df.columns)}\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def clean_salary_player_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the proper 'First Last' name from the salary file's\n",
    "    'Last First Last' format (e.g., \"Young Trae Young\").\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Split ONLY on the first space\n",
    "        # \"Young Trae Young\" -> [\"Young\", \"Trae Young\"]\n",
    "        # \"Nance Jr. Larry Nance Jr.\" -> [\"Nance\", \"Jr. Larry Nance Jr.\"]\n",
    "        # \"Dick Gradey Dick\" -> [\"Dick\", \"Gradey Dick\"]\n",
    "        parts = name.split(' ', 1)\n",
    "        \n",
    "        if len(parts) == 2:\n",
    "            # Return the second part, which is the \"Firstname Lastname\"\n",
    "            return parts[1]\n",
    "        else:\n",
    "            # If there's no space, just return the name as-is\n",
    "            return name\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not clean salary name '{name}'. Error: {e}\")\n",
    "        return name\n",
    "\n",
    "def clean_team_id(df: pd.DataFrame, col_name='TEAM_ID') -> pd.DataFrame:\n",
    "    \"\"\"Converts TEAM_ID column to a standardized integer format for merging.\"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        print(f\"--- ðŸ”´ ERROR: Tried to clean '{col_name}' but column does not exist.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "        # Drop rows where TEAM_ID could not be converted (became <NA>)\n",
    "        rows_before = len(df)\n",
    "        df = df.dropna(subset=[col_name])\n",
    "        rows_after = len(df)\n",
    "        if rows_before > rows_after:\n",
    "            print(f\"âš ï¸ Dropped {rows_before - rows_after} rows with invalid/missing TEAM_ID.\")\n",
    "        \n",
    "        df[col_name] = df[col_name].astype('Int64')\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"--- ðŸ”´ ERROR: Could not convert '{col_name}' to a number for merging. ---\")\n",
    "        print(f\"Error: {e}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to merge all PLAYER-related data.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Player Merge Process ---\")\n",
    "\n",
    "    # --- Part 1: Load Player Data ---\n",
    "    print(\"\\n--- [Step 1] Loading Player Data ---\")\n",
    "    # Note: Using the column names from your output\n",
    "    df_stats = load_data(STATS_FILE, ['PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID'])\n",
    "    df_context = load_data(CONTEXT_FILE, ['PLAYER_ID', 'PLAYER_NAME', 'BIRTHDATE'])\n",
    "    df_salary = load_data(SALARY_FILE, ['Player_Name', 'Salary'])\n",
    "    df_popularity = load_data(POPULARITY_FILE, ['Player', 'Followers'])\n",
    "\n",
    "    # --- Part 2: Merge Stats + Context ---\n",
    "    print(\"\\n--- [Step 2] Merging Stats and Context (on PLAYER_ID) ---\")\n",
    "    rows_stats = len(df_stats)\n",
    "    rows_context = len(df_context)\n",
    "    \n",
    "    # Using 'inner' merge as requested\n",
    "    df_player_base = pd.merge(\n",
    "        df_stats, \n",
    "        df_context, \n",
    "        on=\"PLAYER_ID\", \n",
    "        how=\"inner\",\n",
    "        suffixes=('_stats', '_context')\n",
    "    )\n",
    "    \n",
    "    print(f\"Stats rows: {rows_stats} | Context rows: {rows_context}\")\n",
    "    print(f\"Merge complete. Result rows: {len(df_player_base)}\")\n",
    "    \n",
    "    if len(df_player_base) == 0:\n",
    "        print(\"--- ðŸ”´ ERROR: Merge 1 (Stats + Context) resulted in 0 rows. ---\")\n",
    "        print(\"Please check PLAYER_ID columns in both files.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # --- Part 3: Merge Salaries ---\n",
    "    print(\"\\n--- [Step 3] Merging Salaries (on Standardized Name) ---\")\n",
    "    \n",
    "    # Standardize names for merging\n",
    "    # Note: Use the PLAYER_NAME from the context file if it exists and is cleaner\n",
    "    df_player_base['merge_key'] = df_player_base['PLAYER_NAME_context'].apply(standardize_player_name)\n",
    "    \n",
    "    # --- THIS IS THE FIX ---\n",
    "    # 1. Clean the salary name structure (e.g., \"Young Trae Young\" -> \"Trae Young\")\n",
    "    # 2. Standardize the *cleaned* name (e.g., \"Trae Young\" -> \"trae young\")\n",
    "    df_salary['merge_key'] = df_salary['Player_Name'].apply(clean_salary_player_name).apply(standardize_player_name)\n",
    "    # --- END FIX ---\n",
    "    \n",
    "    # --- DEBUGGING PRINT STATEMENTS ---\n",
    "    print(\"\\n--- DEBUG: Checking standardized names for merge ---\")\n",
    "    print(\"\\n--- Names from Player Base (Stats/Context): ---\")\n",
    "    print(df_player_base[['PLAYER_NAME_context', 'merge_key']].head(15).to_string())\n",
    "    \n",
    "    print(\"\\n--- Names from Salary File: ---\")\n",
    "    print(df_salary[['Player_Name', 'merge_key']].head(15).to_string())\n",
    "    print(\"--- End Debug ---\")\n",
    "    # --- END DEBUGGING ---\n",
    "    \n",
    "    rows_before_salary = len(df_player_base)\n",
    "    rows_salary = len(df_salary)\n",
    "    \n",
    "    # Using 'inner' merge as requested in your script\n",
    "    # We can change to 'left' if you want to keep players without salary info\n",
    "    df_player_master = pd.merge(\n",
    "        df_player_base, \n",
    "        df_salary, \n",
    "        on=\"merge_key\", \n",
    "        how=\"inner\" # Change to \"left\" to keep all players\n",
    "    )\n",
    "    \n",
    "    print(f\"Player Base rows: {rows_before_salary} | Salary rows: {rows_salary}\")\n",
    "    print(f\"Merge complete. Result rows: {len(df_player_master)}\")\n",
    "    print(f\"âš ï¸ Players dropped (no salary match): {rows_before_salary - len(df_player_master)}\")\n",
    "    \n",
    "    if len(df_player_master) == 0:\n",
    "        print(\"--- ðŸ”´ ERROR: Merge 2 (Salaries) resulted in 0 rows. ---\")\n",
    "        print(\"Check name standardization or if salary file is correct.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # --- Part 4: Merge Player Popularity (NEW) ---\n",
    "    print(\"\\n--- [Step 4] Merging Player Popularity (on Standardized Name) ---\")\n",
    "    df_popularity['merge_key'] = df_popularity['Player'].apply(standardize_player_name)\n",
    "    \n",
    "    rows_before_pop = len(df_player_master)\n",
    "    rows_pop = len(df_popularity)\n",
    "    \n",
    "    # Use LEFT merge to keep all players, even if not on popularity list\n",
    "    df_player_master = pd.merge(\n",
    "        df_player_master,\n",
    "        df_popularity.drop(columns=['Player'], errors='ignore'), # Drop original name col\n",
    "        on=\"merge_key\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Player Master rows: {rows_before_pop} | Popularity rows: {rows_pop}\")\n",
    "    print(f\"Left merge complete. Result rows: {len(df_player_master)}\")\n",
    "    print(f\"Players kept (from left merge): {len(df_player_master)}\")\n",
    "\n",
    "    # --- Part 5: Final Cleanup and Save ---\n",
    "    print(\"\\n--- [Step 5] Cleaning and Saving Player Dataset ---\")\n",
    "    \n",
    "    # Clean up salary/cap/numeric columns\n",
    "    numeric_cols_to_clean = [\n",
    "        'Salary', 'Followers'\n",
    "    ]\n",
    "    \n",
    "    for col in numeric_cols_to_clean:\n",
    "        if col in df_player_master.columns:\n",
    "            df_player_master[col] = (\n",
    "                df_player_master[col]\n",
    "                .astype(str)\n",
    "                .str.replace(r\"[^\\d.]\", \"\", regex=True) # Keep digits and decimals\n",
    "                .replace(\"\", None)\n",
    "                .astype(float)\n",
    "            )\n",
    "    \n",
    "    # Save to output file\n",
    "    output_path = Path(OUTPUT_FILE)\n",
    "    df_player_master.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… --- PLAYER MERGE COMPLETE --- âœ…\")\n",
    "    print(f\"Final player dataset with {len(df_player_master)} rows saved to:\")\n",
    "    print(f\"{output_path.resolve()}\")\n",
    "    \n",
    "    print(\"\\n--- NEXT STEP ---\")\n",
    "    print(\"We now need to merge the team data (Caps, Owners, Stadiums).\")\n",
    "    print(\"This will require creating a mapping between 'TEAM_ID', 'team' (abbreviation), and 'Team Name'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0eb10a",
   "metadata": {},
   "source": [
    "# now with team ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acd2f1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Full Merge Process ---\n",
      "\n",
      "--- [Step 1] Loading Player Data ---\n",
      "Attempting to load file: '../../data/raw/Player_Performance_raw.csv'...\n",
      "âœ… Successfully loaded '../../data/raw/Player_Performance_raw.csv'. Found 569 rows.\n",
      "Attempting to load file: '../../data/raw/raw_player_context.csv'...\n",
      "âœ… Successfully loaded '../../data/raw/raw_player_context.csv'. Found 569 rows.\n",
      "Attempting to load file: '../../data/raw/raw_player_salaries.csv'...\n",
      "âœ… Successfully loaded '../../data/raw/raw_player_salaries.csv'. Found 450 rows.\n",
      "Attempting to load file: '../../data/raw/nba_player_popularity.csv'...\n",
      "âœ… Successfully loaded '../../data/raw/nba_player_popularity.csv'. Found 511 rows.\n",
      "\n",
      "--- [Step 2] Building Player Master List ---\n",
      "Merging Stats + Context...\n",
      "Merging Salaries...\n",
      "Merge complete. Result rows: 409\n",
      "âš ï¸ Players dropped (no salary match): 160\n",
      "Merging Player Popularity...\n",
      "Player Master list complete. Total rows: 409\n",
      "\n",
      "--- [Step 3] Loading Team Data ---\n",
      "Attempting to load file: '../../data/raw/raw_salary_caps.csv'...\n",
      "âœ… Successfully loaded '../../data/raw/raw_salary_caps.csv'. Found 30 rows.\n",
      "Attempting to load file: '../../data/raw/Owner Net Worth in Billions .csv'...\n",
      "âœ… Successfully loaded '../../data/raw/Owner Net Worth in Billions .csv'. Found 32 rows.\n",
      "Attempting to load file: '../../data/raw/nba_stadiums.csv'...\n",
      "âœ… Successfully loaded '../../data/raw/nba_stadiums.csv'. Found 30 rows.\n",
      "\n",
      "--- [Step 4] Building Team Master List ---\n",
      "âš ï¸ Dropped 2 rows with invalid/missing TEAM_ID in column 'TEAM_ID'.\n",
      "Merging Caps + Owners...\n",
      "Team Base (Caps+Owners) complete. Result rows: 29\n",
      "Merging Stadium Data...\n",
      "Team Master list complete. Total rows: 29\n",
      "\n",
      "--- [Step 5] Final Merge: Players + Teams (on TEAM_ID) ---\n",
      "Player Master rows: 409 | Team Master rows: 29\n",
      "Final merge complete. Final rows: 409\n",
      "\n",
      "--- [Step 6] Cleaning and Saving Final Dataset ---\n",
      "\n",
      "âœ… --- PROCESS COMPLETE --- âœ…\n",
      "Final dataset with 409 rows and 86 columns saved to:\n",
      "C:\\Users\\tyler\\School\\Learn Statistics\\STA 160\\Project\\notebooks\\Tyler\\master_dataset_v3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Temp\\ipykernel_30616\\2160656193.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col_name] = df[col_name].astype('Int64')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# --- File Configuration ---\n",
    "BASE_PATH = '../../data/raw/' # Set the base path for all files\n",
    "\n",
    "# Player files\n",
    "STATS_FILE = os.path.join(BASE_PATH, 'Player_Performance_raw.csv')\n",
    "CONTEXT_FILE = os.path.join(BASE_PATH, 'raw_player_context.csv')\n",
    "SALARY_FILE = os.path.join(BASE_PATH, 'raw_player_salaries.csv')\n",
    "POPULARITY_FILE = os.path.join(BASE_PATH, 'nba_player_popularity.csv')\n",
    "\n",
    "# Team files\n",
    "CAPS_FILE = os.path.join(BASE_PATH, 'raw_salary_caps.csv')\n",
    "STADIUMS_FILE = os.path.join(BASE_PATH, 'nba_stadiums.csv')\n",
    "OWNERS_FILE = os.path.join(BASE_PATH, 'Owner Net Worth in Billions .csv')\n",
    "\n",
    "# Final output file\n",
    "OUTPUT_FILE = 'master_dataset_v3.csv'\n",
    "\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "def standardize_player_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans and standardizes player names so merges work across data sources.\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize unicode (e.g., accents)\n",
    "    try:\n",
    "        name = str(name)\n",
    "        name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not normalize name '{name}'. Error: {e}\")\n",
    "        pass\n",
    "\n",
    "    # Replace punctuation and collapse spaces\n",
    "    name = re.sub(r'[^a-zA-Z\\s]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    \n",
    "    return name.strip().lower()\n",
    "\n",
    "\n",
    "def load_data(filename: str, required_cols: list = None, dtype: dict = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a CSV file with robust debugging and error checking.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to load file: '{filename}'...\")\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"--- ðŸ”´ ERROR: File not found! ---\")\n",
    "        print(f\"Script stopped. Could not find file: {filename}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    try:\n",
    "        # Use skipinitialspace=True to handle spaces in column names\n",
    "        df = pd.read_csv(filename, skipinitialspace=True, dtype=dtype)\n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        print(f\"âœ… Successfully loaded '{filename}'. Found {len(df)} rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"--- ðŸ”´ ERROR: Could not read file! ---\")\n",
    "        print(f\"Could not load {filename}. Error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if required_cols:\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"--- ðŸ”´ ERROR: Missing required columns in '{filename}'! ---\")\n",
    "            print(f\"Expected columns: {required_cols}\")\n",
    "            print(f\"Missing columns: {missing_cols}\")\n",
    "            print(f\"All columns found: {list(df.columns)}\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def clean_salary_player_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the proper 'First Last' name from the salary file's\n",
    "    'Last First Last' format (e.g., \"Young Trae Young\").\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Split ONLY on the first space\n",
    "        parts = name.split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            return parts[1] # Return the \"Firstname Lastname\" part\n",
    "        else:\n",
    "            return name\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not clean salary name '{name}'. Error: {e}\")\n",
    "        return name\n",
    "\n",
    "def clean_team_id(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Converts a specified TEAM_ID column to a standardized integer format.\"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        print(f\"--- ðŸ”´ ERROR: Tried to clean '{col_name}' but column does not exist.\")\n",
    "        print(f\"All columns found: {list(df.columns)}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "        rows_before = len(df)\n",
    "        df = df.dropna(subset=[col_name])\n",
    "        rows_after = len(df)\n",
    "        if rows_before > rows_after:\n",
    "            print(f\"âš ï¸ Dropped {rows_before - rows_after} rows with invalid/missing TEAM_ID in column '{col_name}'.\")\n",
    "        \n",
    "        df[col_name] = df[col_name].astype('Int64')\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"--- ðŸ”´ ERROR: Could not convert '{col_name}' to a number for merging. ---\")\n",
    "        print(f\"Error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def clean_numeric_cols(df: pd.DataFrame, cols_to_clean: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans specified columns by removing non-numeric characters\n",
    "    and converting them to floats.\n",
    "    \"\"\"\n",
    "    for col in cols_to_clean:\n",
    "        if col in df.columns:\n",
    "            df[col] = (\n",
    "                df[col]\n",
    "                .astype(str)\n",
    "                .str.replace(r\"[^\\d.]\", \"\", regex=True) # Keep digits and decimals\n",
    "                .replace(\"\", None)\n",
    "                .astype(float)\n",
    "            )\n",
    "    return df\n",
    "        \n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the entire merge workflow.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Full Merge Process ---\")\n",
    "\n",
    "    # --- Part 1: Load Player Data ---\n",
    "    print(\"\\n--- [Step 1] Loading Player Data ---\")\n",
    "    df_stats = load_data(STATS_FILE, ['PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID'])\n",
    "    df_context = load_data(CONTEXT_FILE, ['PLAYER_ID', 'PLAYER_NAME'])\n",
    "    df_salary = load_data(SALARY_FILE, ['Player_Name', 'Salary'])\n",
    "    df_popularity = load_data(POPULARITY_FILE, ['Player', 'Followers'])\n",
    "\n",
    "    # --- Part 2: Build Player Master DataFrame ---\n",
    "    print(\"\\n--- [Step 2] Building Player Master List ---\")\n",
    "    \n",
    "    # Merge Stats + Context (on PLAYER_ID)\n",
    "    print(\"Merging Stats + Context...\")\n",
    "    df_player_master = pd.merge(\n",
    "        df_stats, \n",
    "        df_context, \n",
    "        on=\"PLAYER_ID\", \n",
    "        how=\"inner\",\n",
    "        suffixes=('_stats', '_context')\n",
    "    )\n",
    "    \n",
    "    # Merge Salaries (on Standardized Name)\n",
    "    print(\"Merging Salaries...\")\n",
    "    df_player_master['merge_key'] = df_player_master['PLAYER_NAME_context'].apply(standardize_player_name)\n",
    "    df_salary['merge_key'] = df_salary['Player_Name'].apply(clean_salary_player_name).apply(standardize_player_name)\n",
    "    \n",
    "    rows_before_salary = len(df_player_master)\n",
    "    df_player_master = pd.merge(\n",
    "        df_player_master, \n",
    "        df_salary, \n",
    "        on=\"merge_key\", \n",
    "        how=\"inner\" # Keep only players with salary info\n",
    "    )\n",
    "    print(f\"Merge complete. Result rows: {len(df_player_master)}\")\n",
    "    print(f\"âš ï¸ Players dropped (no salary match): {rows_before_salary - len(df_player_master)}\")\n",
    "\n",
    "    # Merge Player Popularity (on Standardized Name)\n",
    "    print(\"Merging Player Popularity...\")\n",
    "    df_popularity['merge_key'] = df_popularity['Player'].apply(standardize_player_name)\n",
    "    \n",
    "    df_player_master = pd.merge(\n",
    "        df_player_master,\n",
    "        df_popularity.drop(columns=['Player'], errors='ignore'),\n",
    "        on=\"merge_key\",\n",
    "        how=\"left\" # Keep all players, even if no popularity info\n",
    "    )\n",
    "    print(f\"Player Master list complete. Total rows: {len(df_player_master)}\")\n",
    "\n",
    "    # --- Part 3: Load Team Data ---\n",
    "    print(\"\\n--- [Step 3] Loading Team Data ---\")\n",
    "    df_caps = load_data(CAPS_FILE, ['team_id', 'team'])\n",
    "    df_owners = load_data(OWNERS_FILE, ['Team ID', 'Team Name'])\n",
    "    df_stadiums = load_data(STADIUMS_FILE, ['TEAM_ABBREVIATION', 'Stadium_Name'])\n",
    "\n",
    "    # --- Part 4: Build Team Master DataFrame ---\n",
    "    print(\"\\n--- [Step 4] Building Team Master List ---\")\n",
    "    \n",
    "    # Standardize TEAM_ID column names and types BEFORE merging\n",
    "    df_caps = df_caps.rename(columns={'team_id': 'TEAM_ID'})\n",
    "    # FIX: Drop duplicates to ensure only one row per team\n",
    "    df_caps = clean_team_id(df_caps, 'TEAM_ID').drop_duplicates(subset=['TEAM_ID'])\n",
    "    \n",
    "    df_owners = df_owners.rename(columns={'Team ID': 'TEAM_ID'})\n",
    "    # FIX: Drop duplicates to ensure only one row per team\n",
    "    df_owners = clean_team_id(df_owners, 'TEAM_ID').drop_duplicates(subset=['TEAM_ID'])\n",
    "    \n",
    "    # Drop junk column\n",
    "    if 'Unnamed: 3' in df_owners.columns:\n",
    "        df_owners = df_owners.drop(columns=['Unnamed: 3'])\n",
    "\n",
    "    # Merge Caps + Owners (on TEAM_ID)\n",
    "    print(\"Merging Caps + Owners...\")\n",
    "    df_team_master = pd.merge(\n",
    "        df_caps,\n",
    "        df_owners,\n",
    "        on=\"TEAM_ID\",\n",
    "        how=\"inner\" # Use 'inner' to keep only teams present in both\n",
    "    )\n",
    "    print(f\"Team Base (Caps+Owners) complete. Result rows: {len(df_team_master)}\")\n",
    "\n",
    "    # Merge Stadiums (on Abbreviation)\n",
    "    print(\"Merging Stadium Data...\")\n",
    "    df_team_master = pd.merge(\n",
    "        df_team_master,\n",
    "        df_stadiums,\n",
    "        left_on=\"team\", # Abbreviation from raw_salary_caps.csv\n",
    "        right_on=\"TEAM_ABBREVIATION\",\n",
    "        how=\"left\" # Keep all teams, even if no stadium info\n",
    "    )\n",
    "    print(f\"Team Master list complete. Total rows: {len(df_team_master)}\")\n",
    "\n",
    "    # --- Part 5: Final Merge: Players + Teams ---\n",
    "    print(\"\\n--- [Step 5] Final Merge: Players + Teams (on TEAM_ID) ---\")\n",
    "    \n",
    "    # FIX: Use .copy() to prevent SettingWithCopyWarning\n",
    "    df_player_master_clean = clean_team_id(df_player_master.copy(), 'TEAM_ID')\n",
    "    \n",
    "    rows_players = len(df_player_master_clean)\n",
    "    rows_teams = len(df_team_master)\n",
    "    \n",
    "    df_final = pd.merge(\n",
    "        df_player_master_clean,\n",
    "        df_team_master,\n",
    "        on=\"TEAM_ID\",\n",
    "        how=\"left\" # Keep ALL players, attach team info\n",
    "    )\n",
    "    \n",
    "    print(f\"Player Master rows: {rows_players} | Team Master rows: {rows_teams}\")\n",
    "    print(f\"Final merge complete. Final rows: {len(df_final)}\")\n",
    "    \n",
    "    if len(df_final) != rows_players:\n",
    "        print(f\"âš ï¸ Warning: Row count changed. Investigate merge. Expected {rows_players}.\")\n",
    "\n",
    "    # --- Part 6: Final Cleanup and Save ---\n",
    "    print(\"\\n--- [Step 6] Cleaning and Saving Final Dataset ---\")\n",
    "    \n",
    "    # Rename messy 'Owner Net Worth' column\n",
    "    if 'Owner Net Worth in Billions' in df_final.columns:\n",
    "        df_final = df_final.rename(columns={\n",
    "            'Owner Net Worth in Billions': 'Owner_Net_Worth_Billions'\n",
    "        })\n",
    "    \n",
    "    # List of ALL numeric columns that need cleaning\n",
    "    all_numeric_cols = [\n",
    "        'Salary', 'Followers', 'total_cap_used', 'remaining_cap_space', \n",
    "        'active_cap', 'active_top_3', 'dead_cap', 'Capacity', \n",
    "        'Construction_Cost', 'Owner_Net_Worth_Billions'\n",
    "    ]\n",
    "    \n",
    "    df_final = clean_numeric_cols(df_final, all_numeric_cols)\n",
    "    \n",
    "    # Save to output file\n",
    "    output_path = Path(OUTPUT_FILE)\n",
    "    df_final.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… --- PROCESS COMPLETE --- âœ…\")\n",
    "    print(f\"Final dataset with {len(df_final)} rows and {len(df_final.columns)} columns saved to:\")\n",
    "    print(f\"{output_path.resolve()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0030d4",
   "metadata": {},
   "source": [
    "# Now that we merged, we clean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2fb839",
   "metadata": {},
   "source": [
    "Print columns and missing data. Some columns are duplicates of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ee5f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing 'master_dataset.csv' ---\n",
      "âœ… Successfully loaded. Found 409 rows.\n",
      "\n",
      "--- Column List (Total: 86) ---\n",
      "1. PLAYER_ID\n",
      "2. PLAYER_NAME_stats\n",
      "3. TEAM_ID\n",
      "4. E_OFF_RATING\n",
      "5. OFF_RATING\n",
      "6. sp_work_OFF_RATING\n",
      "7. E_DEF_RATING\n",
      "8. DEF_RATING\n",
      "9. sp_work_DEF_RATING\n",
      "10. E_NET_RATING\n",
      "11. NET_RATING\n",
      "12. sp_work_NET_RATING\n",
      "13. AST_PCT\n",
      "14. AST_TO\n",
      "15. AST_RATIO\n",
      "16. OREB_PCT\n",
      "17. DREB_PCT\n",
      "18. REB_PCT\n",
      "19. TM_TOV_PCT\n",
      "20. E_TOV_PCT\n",
      "21. EFG_PCT\n",
      "22. TS_PCT\n",
      "23. USG_PCT\n",
      "24. E_USG_PCT\n",
      "25. E_PACE\n",
      "26. PACE\n",
      "27. PACE_PER40\n",
      "28. sp_work_PACE\n",
      "29. PIE\n",
      "30. POSS\n",
      "31. FGM_PG\n",
      "32. FGA_PG\n",
      "33. E_OFF_RATING_RANK\n",
      "34. OFF_RATING_RANK\n",
      "35. sp_work_OFF_RATING_RANK\n",
      "36. E_DEF_RATING_RANK\n",
      "37. DEF_RATING_RANK\n",
      "38. sp_work_DEF_RATING_RANK\n",
      "39. E_NET_RATING_RANK\n",
      "40. NET_RATING_RANK\n",
      "41. sp_work_NET_RATING_RANK\n",
      "42. AST_PCT_RANK\n",
      "43. AST_TO_RANK\n",
      "44. AST_RATIO_RANK\n",
      "45. OREB_PCT_RANK\n",
      "46. DREB_PCT_RANK\n",
      "47. REB_PCT_RANK\n",
      "48. TM_TOV_PCT_RANK\n",
      "49. E_TOV_PCT_RANK\n",
      "50. EFG_PCT_RANK\n",
      "51. TS_PCT_RANK\n",
      "52. USG_PCT_RANK\n",
      "53. E_USG_PCT_RANK\n",
      "54. E_PACE_RANK\n",
      "55. PACE_RANK\n",
      "56. sp_work_PACE_RANK\n",
      "57. PIE_RANK\n",
      "58. FGM_PG_RANK\n",
      "59. FGA_PG_RANK\n",
      "60. PLAYER_NAME_context\n",
      "61. BIRTHDATE\n",
      "62. COUNTRY\n",
      "63. DRAFT_YEAR\n",
      "64. DRAFT_ROUND\n",
      "65. DRAFT_NUMBER\n",
      "66. merge_key\n",
      "67. Player_Name\n",
      "68. Salary\n",
      "69. Followers\n",
      "70. team\n",
      "71. record\n",
      "72. active_players\n",
      "73. avg_team_age\n",
      "74. total_cap_used\n",
      "75. remaining_cap_space\n",
      "76. active_cap\n",
      "77. active_top_3\n",
      "78. dead_cap\n",
      "79. Team Name\n",
      "80. Owner_Net_Worth_Billions\n",
      "81. TEAM_ABBREVIATION\n",
      "82. Stadium_Name\n",
      "83. Capacity\n",
      "84. City\n",
      "85. Year_Opened\n",
      "86. Construction_Cost\n",
      "\n",
      "--- Missing Data Summary ---\n",
      "Found 20 columns with missing data:\n",
      "dead_cap                    79\n",
      "Followers                   44\n",
      "record                      13\n",
      "team                        13\n",
      "Team Name                   13\n",
      "Owner_Net_Worth_Billions    13\n",
      "avg_team_age                13\n",
      "active_players              13\n",
      "total_cap_used              13\n",
      "remaining_cap_space         13\n",
      "active_cap                  13\n",
      "active_top_3                13\n",
      "Capacity                    13\n",
      "City                        13\n",
      "TEAM_ABBREVIATION           13\n",
      "Stadium_Name                13\n",
      "Year_Opened                 13\n",
      "Construction_Cost           13\n",
      "DRAFT_NUMBER                 2\n",
      "DRAFT_ROUND                  1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the master file you just uploaded\n",
    "DATA_FILE = 'master_dataset.csv'\n",
    "\n",
    "def analyze_columns(filename: str):\n",
    "    \"\"\"\n",
    "    Loads the dataset and prints a numbered list of all columns\n",
    "    and checks for missing data.\n",
    "    \"\"\"\n",
    "    print(f\"--- Analyzing '{filename}' ---\")\n",
    "\n",
    "    # 1. Check if file exists\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"ðŸ”´ ERROR: File not found: {filename}\")\n",
    "        print(\"Please make sure the file is in the same directory as the script.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        # Set low_memory=False as a precaution for mixed data types in large files\n",
    "        df = pd.read_csv(filename, low_memory=False)\n",
    "        print(f\"âœ… Successfully loaded. Found {len(df)} rows.\")\n",
    "\n",
    "        # 2. Print all column names\n",
    "        print(f\"\\n--- Column List (Total: {len(df.columns)}) ---\")\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            print(f\"{i}. {col}\")\n",
    "        \n",
    "        # 3. Print a summary of missing data\n",
    "        print(\"\\n--- Missing Data Summary ---\")\n",
    "        missing_counts = df.isnull().sum()\n",
    "        missing_cols = missing_counts[missing_counts > 0]\n",
    "        \n",
    "        if missing_cols.empty:\n",
    "            print(\"âœ… No missing data found!\")\n",
    "        else:\n",
    "            print(f\"Found {len(missing_cols)} columns with missing data:\")\n",
    "            # Sort by most missing to least\n",
    "            print(missing_cols.sort_values(ascending=False))\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"ðŸ”´ ERROR: The file '{filename}' is empty.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ”´ ERROR: Could not read file. Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_columns(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3f3a6f",
   "metadata": {},
   "source": [
    "# remove rendundant features and ranks. intelligently group players by teamid and fill out any missing team data with the same as their teammates. drop unneccessary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b9da4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Cleaning Process for 'master_dataset.csv' ---\n",
      "âœ… Successfully loaded. Found 409 rows and 86 columns.\n",
      "âœ… Dropped 51 redundant/noise columns.\n",
      "âœ… Renamed 6 columns for clarity.\n",
      "--- Handling Missing Data ---\n",
      "âœ… Filled missing player-specific data.\n",
      "âœ… Smart-filled missing team data by copying from teammates.\n",
      "âœ… Performed final fallback fill for any remaining NaNs.\n",
      "\n",
      "âœ… --- CLEANING COMPLETE --- âœ…\n",
      "Final dataset with 409 rows and 45 columns saved to:\n",
      "C:\\Users\\tyler\\School\\Learn Statistics\\STA 160\\Project\\notebooks\\Tyler\\master_dataset_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "SOURCE_FILE = 'master_dataset.csv'\n",
    "OUTPUT_FILE = 'master_dataset_cleaned.csv'\n",
    "\n",
    "def clean_master_dataset(filename: str, output_filename: str):\n",
    "    \"\"\"\n",
    "    Loads the master dataset, drops redundant columns, renames others,\n",
    "    handles missing data, and saves a cleaned version.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Data Cleaning Process for '{filename}' ---\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"ðŸ”´ ERROR: File not found: {filename}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filename, low_memory=False)\n",
    "        print(f\"âœ… Successfully loaded. Found {len(df)} rows and {len(df.columns)} columns.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ”´ ERROR: Could not read file. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Define Columns to Drop\n",
    "    # Based on our brainstorming session\n",
    "    \n",
    "    # Redundant helper/merge columns\n",
    "    cols_to_drop = [\n",
    "        'PLAYER_NAME_stats',     # Redundant name\n",
    "        'Player_Name',           # Messy salary name\n",
    "        'merge_key',             # Helper column\n",
    "        'TEAM_ABBREVIATION',     # Duplicate of 'team'\n",
    "    ]\n",
    "\n",
    "    # Redundant \"Estimated\" (E_) metric set\n",
    "    e_cols = [col for col in df.columns if col.startswith('E_')]\n",
    "    cols_to_drop.extend(e_cols)\n",
    "\n",
    "    # Redundant \"sp_work_\" metric set\n",
    "    sp_work_cols = [col for col in df.columns if col.startswith('sp_work_')]\n",
    "    cols_to_drop.extend(sp_work_cols)\n",
    "    \n",
    "    # Redundant \"RANK\" columns\n",
    "    rank_cols = [col for col in df.columns if col.endswith('_RANK')]\n",
    "    cols_to_drop.extend(rank_cols)\n",
    "    \n",
    "    # Check for columns that might not exist, just in case\n",
    "    actual_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
    "    \n",
    "    df_cleaned = df.drop(columns=actual_cols_to_drop)\n",
    "    print(f\"âœ… Dropped {len(actual_cols_to_drop)} redundant/noise columns.\")\n",
    "\n",
    "    # 3. Rename Columns for Clarity\n",
    "    cols_to_rename = {\n",
    "        'PLAYER_NAME_context': 'PLAYER_NAME',\n",
    "        'Team Name': 'TEAM_NAME',\n",
    "        'team': 'TEAM_ABBREVIATION',\n",
    "        'Owner_Net_Worth_Billions': 'OWNER_NET_WORTH_B',\n",
    "        'Construction_Cost': 'STADIUM_COST',\n",
    "        'Year_Opened': 'STADIUM_YEAR_OPENED'\n",
    "    }\n",
    "    # Only rename columns that actually exist\n",
    "    actual_cols_to_rename = {k: v for k, v in cols_to_rename.items() if k in df_cleaned.columns}\n",
    "    df_cleaned = df_cleaned.rename(columns=actual_cols_to_rename)\n",
    "    print(f\"âœ… Renamed {len(actual_cols_to_rename)} columns for clarity.\")\n",
    "\n",
    "    # 4. Handle Missing Data\n",
    "    print(\"--- Handling Missing Data ---\")\n",
    "    \n",
    "    # --- Handle Player-Specific Data (Fill with 0 or \"Undrafted\") ---\n",
    "    # This logic remains the same.\n",
    "    fill_zero_cols = ['Followers'] # Only fill followers with 0\n",
    "    for col in fill_zero_cols:\n",
    "        if col in df_cleaned.columns:\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(0)\n",
    "            \n",
    "    if 'DRAFT_ROUND' in df_cleaned.columns:\n",
    "        df_cleaned['DRAFT_ROUND'] = df_cleaned['DRAFT_ROUND'].fillna('Undrafted')\n",
    "        \n",
    "    if 'DRAFT_NUMBER' in df_cleaned.columns:\n",
    "        df_cleaned['DRAFT_NUMBER'] = df_cleaned['DRAFT_NUMBER'].fillna('Undrafted')\n",
    "    \n",
    "    print(\"âœ… Filled missing player-specific data.\")\n",
    "\n",
    "    # --- Handle Missing Team Data (User's new logic) ---\n",
    "    # This is the new, smarter logic you suggested.\n",
    "    # We copy team data from a teammate (same TEAM_ID) to fill gaps.\n",
    "    team_cols = [\n",
    "        'TEAM_ABBREVIATION', 'record', 'active_players', 'avg_team_age',\n",
    "        'total_cap_used', 'remaining_cap_space', 'active_cap',\n",
    "        'active_top_3', 'dead_cap', 'TEAM_NAME', 'OWNER_NET_WORTH_B',\n",
    "        'Stadium_Name', 'Capacity', 'City', 'STADIUM_YEAR_OPENED', 'STADIUM_COST'\n",
    "    ]\n",
    "    \n",
    "    # Filter for columns that actually exist in the dataframe\n",
    "    existing_team_cols = [col for col in team_cols if col in df_cleaned.columns]\n",
    "    \n",
    "    # Group by TEAM_ID and fill NaNs with values from other rows in that same group\n",
    "    # .ffill() fills forward, .bfill() fills backward to catch all gaps.\n",
    "    df_cleaned[existing_team_cols] = df_cleaned.groupby('TEAM_ID')[existing_team_cols].ffill().bfill()\n",
    "    \n",
    "    print(\"âœ… Smart-filled missing team data by copying from teammates.\")\n",
    "\n",
    "    # --- Handle any remaining NaNs (e.g., a team had NO data) ---\n",
    "    # After smart-filling, if any NaNs are left (e.g., a whole team was missing),\n",
    "    # we fill them with 0 or \"Unknown\" as a final fallback.\n",
    "    \n",
    "    # Numeric team cols\n",
    "    numeric_team_cols = [\n",
    "        'active_players', 'avg_team_age', 'total_cap_used', 'remaining_cap_space', \n",
    "        'active_cap', 'active_top_3', 'dead_cap', 'Capacity', 'STADIUM_COST', \n",
    "        'STADIUM_YEAR_OPENED', 'OWNER_NET_WORTH_B'\n",
    "    ]\n",
    "    for col in numeric_team_cols:\n",
    "        if col in df_cleaned.columns:\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(0)\n",
    "            \n",
    "    # Text team cols\n",
    "    text_team_cols = ['TEAM_ABBREVIATION', 'record', 'TEAM_NAME', 'Stadium_Name', 'City']\n",
    "    for col in text_team_cols:\n",
    "        if col in df_cleaned.columns:\n",
    "            df_cleaned[col] = df_cleaned[col].fillna('Unknown')\n",
    "\n",
    "    print(\"âœ… Performed final fallback fill for any remaining NaNs.\")\n",
    "\n",
    "    # 5. Save Cleaned File\n",
    "    output_path = Path(output_filename)\n",
    "    df_cleaned.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… --- CLEANING COMPLETE --- âœ…\")\n",
    "    print(f\"Final dataset with {len(df_cleaned)} rows and {len(df_cleaned.columns)} columns saved to:\")\n",
    "    print(f\"{output_path.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_master_dataset(SOURCE_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e0b19b",
   "metadata": {},
   "source": [
    "### Data Dictionary: `master_dataset_cleaned.csv`\n",
    "\n",
    "---\n",
    "\n",
    "#### **Group 1: Player Identity & Context**\n",
    "These columns provide basic, unique information about the player.\n",
    "\n",
    "* **`PLAYER_ID`**: The unique, stable identifier for each player, assigned by the NBA. This is your best \"key\" for any player, as names can be similar.\n",
    "* **`PLAYER_NAME`**: The player's full name, cleaned and standardized (e.g., \"LeBron James\").\n",
    "* **`BIRTHDATE`**: The player's date of birth.\n",
    "* **`COUNTRY`**: The player's home country (e.g., \"USA\", \"France\", \"Serbia\").\n",
    "\n",
    "---\n",
    "\n",
    "#### **Group 2: Player Contract & Popularity**\n",
    "These columns describe the player's professional and financial status.\n",
    "\n",
    "* **`DRAFT_YEAR`**: The year the player was drafted into the NBA.\n",
    "* **`DRAFT_ROUND`**: The round they were drafted in. We filled this with **\"Undrafted\"** if the data was missing.\n",
    "* **`DRAFT_NUMBER`**: The specific draft pick number (e.g., `1` for the first pick). We also filled this with **\"Undrafted\"**.\n",
    "* **`Salary`**: The player's contract salary for the season, cleaned to a single numeric value (e.g., `48728845`).\n",
    "* **`Followers`**: The player's social media follower count. We filled this with **`0`** if the player was not on the popularity list, assuming they were below the tracking threshold.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Group 3: Player On-Court Performance (Advanced Stats)**\n",
    "These are the advanced metrics from the `Player_Performance_raw.csv` file. We kept the standard set and dropped the redundant `E_` and `sp_work_` versions.\n",
    "\n",
    "* **`OFF_RATING`**: **Offensive Rating**. An estimate of the points a player produces per 100 possessions.\n",
    "* **`DEF_RATING`**: **Defensive Rating**. An estimate of the points a player allows per 100 possessions.\n",
    "* **`NET_RATING`**: **Net Rating**. The difference between Offensive and Defensive Rating (`OFF_RATING` - `DEF_RATING`). This is a key metric for a player's overall on-court impact.\n",
    "* **`AST_PCT`**: **Assist Percentage**. An estimate of the percentage of teammate field goals a player assisted on while they were on the floor.\n",
    "* **`AST_TO`**: **Assist to Turnover Ratio**. A simple ratio of how many assists a player has for every one turnover.\n",
    "* **`AST_RATIO`**: **Assist Ratio**. An estimate of assists per 100 of the player's own possessions.\n",
    "* **`OREB_PCT`**: **Offensive Rebound Percentage**. An estimate of the percentage of available offensive rebounds a player grabbed.\n",
    "* **`DREB_PCT`**: **Defensive Rebound Percentage**. An estimate of the percentage of available defensive rebounds a player grabbed.\n",
    "* **`REB_PCT`**: **Total Rebound Percentage**. An estimate of the percentage of all available rebounds a player grabbed.\n",
    "* **`TM_TOV_PCT`**: **Turnover Percentage (TOV%)**. An estimate of turnovers committed per 100 plays.\n",
    "* **`EFG_PCT`**: **Effective Field Goal Percentage**. A shooting percentage that adjusts for the fact that a 3-point field goal is worth one more point than a 2-point field goal.\n",
    "* **`TS_PCT`**: **True Shooting Percentage**. A measure of shooting efficiency that accounts for 2-point field goals, 3-point field goals, and free throws. \n",
    "* **`USG_PCT`**: **Usage Percentage**. An estimate of the percentage of team plays \"used\" by a player (i.e., they took a shot, went to the free-throw line, or committed a turnover) while they were on the floor.\n",
    "* **`PACE`**: **Pace**. An estimate of the number of possessions the player's team has per 48 minutes while that player is on the court.\n",
    "* **`PACE_PER40`**: **Pace per 40 Minutes**. A normalized version of the Pace metric.\n",
    "* **`PIE`**: **Player Impact Estimate**. A comprehensive metric that measures a player's overall positive or negative impact on the game, combining many stats.\n",
    "* **`POSS`**: **Possessions**. The total number of offensive possessions the player was on the court for, used as the denominator for many advanced stats.\n",
    "* **`FGM_PG`**: **Field Goals Made Per Game**.\n",
    "* **`FGA_PG`**: **Field Goals Attempted Per Game**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Group 4: Team Identity & Roster Info**\n",
    "This is the data about the team that *each player belongs to*.\n",
    "\n",
    "* **`TEAM_ID`**: The unique, stable identifier for each *team* (e.g., `1610612747` for the Lakers). This was our key for the final merge.\n",
    "* **`TEAM_NAME`**: The full, formal name of the team (e.g., \"Los Angeles Lakers\").\n",
    "* **`TEAM_ABBREVIATION`**: The common 3-letter abbreviation (e.g., \"LAL\").\n",
    "* **`record`**: The team's win-loss record (e.g., \"48-34\").\n",
    "* **`active_players`**: The number of active players on the team's roster.\n",
    "* **`avg_team_age`**: The average age of the players on the team's roster.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Group 5: Team Financials**\n",
    "This is the detailed salary cap and owner information for the team.\n",
    "\n",
    "* **`total_cap_used`**: Total salary cap space used by the team, including active players, dead cap, etc.\n",
    "* **`remaining_cap_space`**: The amount of cap space the team has left under the NBA's salary cap.\n",
    "* **`active_cap`**: The portion of the cap used only by active, rostered players.\n",
    "* **`active_top_3`**: The combined salary of the team's top 3 highest-paid players, indicating how \"top-heavy\" the roster is.\n",
    "* **`dead_cap`**: Money on the team's salary cap that is being paid to players who are *no longer on the roster*.\n",
    "* **`OWNER_NET_WORTH_B`**: The estimated net worth of the team's primary owner, in billions of dollars.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Group 6: Team Stadium Information**\n",
    "This is the contextual information about the team's home arena. \n",
    "\n",
    "* **`Stadium_Name`**: The official name of the team's home arena (e.g., \"Crypto.com Arena\").\n",
    "* **`Capacity`**: The total seating capacity of the stadium for basketball games.\n",
    "* **`City`**: The city and state where the stadium is located.\n",
    "* **`STADIUM_YEAR_OPENED`**: The year the stadium first opened.\n",
    "* **`STADIUM_COST`**: The original construction cost of the stadium, cleaned to a numeric value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa9276",
   "metadata": {},
   "source": [
    "# OK NOW WORK ON DML PULS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225f1ab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
