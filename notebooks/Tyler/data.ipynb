{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de1f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Listing Columns for Each Dataset ---\n",
      "--- Looking in directory: c:\\Users\\tyler\\School\\Learn Statistics\\STA 160\\Project\\data\\raw ---\n",
      "\n",
      "üìÑ File: ../../data/raw/Player_Performance_raw.csv\n",
      "Columns:\n",
      "  - PLAYER_ID\n",
      "  - PLAYER_NAME\n",
      "  - TEAM_ID\n",
      "  - E_OFF_RATING\n",
      "  - OFF_RATING\n",
      "  - sp_work_OFF_RATING\n",
      "  - E_DEF_RATING\n",
      "  - DEF_RATING\n",
      "  - sp_work_DEF_RATING\n",
      "  - E_NET_RATING\n",
      "  - NET_RATING\n",
      "  - sp_work_NET_RATING\n",
      "  - AST_PCT\n",
      "  - AST_TO\n",
      "  - AST_RATIO\n",
      "  - OREB_PCT\n",
      "  - DREB_PCT\n",
      "  - REB_PCT\n",
      "  - TM_TOV_PCT\n",
      "  - E_TOV_PCT\n",
      "  - EFG_PCT\n",
      "  - TS_PCT\n",
      "  - USG_PCT\n",
      "  - E_USG_PCT\n",
      "  - E_PACE\n",
      "  - PACE\n",
      "  - PACE_PER40\n",
      "  - sp_work_PACE\n",
      "  - PIE\n",
      "  - POSS\n",
      "  - FGM_PG\n",
      "  - FGA_PG\n",
      "  - E_OFF_RATING_RANK\n",
      "  - OFF_RATING_RANK\n",
      "  - sp_work_OFF_RATING_RANK\n",
      "  - E_DEF_RATING_RANK\n",
      "  - DEF_RATING_RANK\n",
      "  - sp_work_DEF_RATING_RANK\n",
      "  - E_NET_RATING_RANK\n",
      "  - NET_RATING_RANK\n",
      "  - sp_work_NET_RATING_RANK\n",
      "  - AST_PCT_RANK\n",
      "  - AST_TO_RANK\n",
      "  - AST_RATIO_RANK\n",
      "  - OREB_PCT_RANK\n",
      "  - DREB_PCT_RANK\n",
      "  - REB_PCT_RANK\n",
      "  - TM_TOV_PCT_RANK\n",
      "  - E_TOV_PCT_RANK\n",
      "  - EFG_PCT_RANK\n",
      "  - TS_PCT_RANK\n",
      "  - USG_PCT_RANK\n",
      "  - E_USG_PCT_RANK\n",
      "  - E_PACE_RANK\n",
      "  - PACE_RANK\n",
      "  - sp_work_PACE_RANK\n",
      "  - PIE_RANK\n",
      "  - FGM_PG_RANK\n",
      "  - FGA_PG_RANK\n",
      "\n",
      "üìÑ File: ../../data/raw/raw_player_context.csv\n",
      "Columns:\n",
      "  - PLAYER_ID\n",
      "  - PLAYER_NAME\n",
      "  - BIRTHDATE\n",
      "  - COUNTRY\n",
      "  - DRAFT_YEAR\n",
      "  - DRAFT_ROUND\n",
      "  - DRAFT_NUMBER\n",
      "\n",
      "üìÑ File: ../../data/raw/raw_player_salaries.csv\n",
      "Columns:\n",
      "  - Player_Name\n",
      "  - Salary\n",
      "\n",
      "üìÑ File: ../../data/raw/raw_salary_caps.csv\n",
      "Columns:\n",
      "  - team\n",
      "  - team_id\n",
      "  - record\n",
      "  - active_players\n",
      "  - avg_team_age\n",
      "  - total_cap_used\n",
      "  - remaining_cap_space\n",
      "  - active_cap\n",
      "  - active_top_3\n",
      "  - dead_cap\n",
      "\n",
      "üìÑ File: ../../data/raw/nba_player_popularity.csv\n",
      "Columns:\n",
      "  - Player\n",
      "  - Followers\n",
      "\n",
      "üìÑ File: ../../data/raw/nba_stadiums.csv\n",
      "Columns:\n",
      "  - TEAM_ABBREVIATION\n",
      "  - Stadium_Name\n",
      "  - Capacity\n",
      "  - City\n",
      "  - Year_Opened\n",
      "  - Construction_Cost\n",
      "\n",
      "üìÑ File: ../../data/raw/Owner Net Worth in Billions .csv\n",
      "Columns:\n",
      "  - Team Name\n",
      "  - Owner Net Worth in Billions\n",
      "  - Team ID\n",
      "  - Unnamed: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os # Import the os module to handle paths correctly\n",
    "\n",
    "def list_all_csv_columns():\n",
    "    \"\"\"\n",
    "    Reads all CSV files in the current directory and prints their column names.\n",
    "    \"\"\"\n",
    "    # Get a list of all CSV files provided by the user\n",
    "    # We'll hardcode the list based on the uploaded files\n",
    "    # to ensure we only read the ones relevant to this task.\n",
    "    \n",
    "    # Base directory where the files are located\n",
    "    base_path = \"../../data/raw/\"\n",
    "    \n",
    "    original_filenames = [\n",
    "        \"Player_Performance_raw.csv\",\n",
    "        \"raw_player_context.csv\",\n",
    "        \"raw_player_salaries.csv\",\n",
    "        \"raw_salary_caps.csv\",\n",
    "        \"nba_player_popularity.csv\",\n",
    "        \"nba_stadiums.csv\",\n",
    "        \"Owner Net Worth in Billions .csv\"\n",
    "    ]\n",
    "    \n",
    "    # Create the full file paths by joining the base path and the filename\n",
    "    filenames = [os.path.join(base_path, f) for f in original_filenames]\n",
    "\n",
    "    print(\"--- Listing Columns for Each Dataset ---\")\n",
    "    print(f\"--- Looking in directory: {os.path.abspath(base_path)} ---\") # Added to show the full path it's trying\n",
    "\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            # Use skipinitialspace=True to handle spaces after delimiters,\n",
    "            # which can mess up column names.\n",
    "            df = pd.read_csv(filename, skipinitialspace=True)\n",
    "            \n",
    "            # Clean up column names by stripping leading/trailing whitespace\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            print(f\"\\nüìÑ File: {filename}\") # This will now print the full path\n",
    "            print(\"Columns:\")\n",
    "            for col in df.columns:\n",
    "                print(f\"  - {col}\")\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"\\n‚ùå ERROR: File not found: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå ERROR: Could not read {filename}. Reason: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    list_all_csv_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a42d2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Player Merge Process ---\n",
      "\n",
      "--- [Step 1] Loading Player Data ---\n",
      "Attempting to load file: '../../data/raw/Player_Performance_raw.csv'...\n",
      "‚úÖ Successfully loaded '../../data/raw/Player_Performance_raw.csv'. Found 569 rows.\n",
      "Attempting to load file: '../../data/raw/raw_player_context.csv'...\n",
      "‚úÖ Successfully loaded '../../data/raw/raw_player_context.csv'. Found 569 rows.\n",
      "Attempting to load file: '../../data/raw/raw_player_salaries.csv'...\n",
      "‚úÖ Successfully loaded '../../data/raw/raw_player_salaries.csv'. Found 450 rows.\n",
      "Attempting to load file: '../../data/raw/nba_player_popularity.csv'...\n",
      "‚úÖ Successfully loaded '../../data/raw/nba_player_popularity.csv'. Found 511 rows.\n",
      "\n",
      "--- [Step 2] Merging Stats and Context (on PLAYER_ID) ---\n",
      "Stats rows: 569 | Context rows: 569\n",
      "Merge complete. Result rows: 569\n",
      "\n",
      "--- [Step 3] Merging Salaries (on Standardized Name) ---\n",
      "\n",
      "--- DEBUG: Checking standardized names for merge ---\n",
      "\n",
      "--- Names from Player Base (Stats/Context): ---\n",
      "   PLAYER_NAME_context          merge_key\n",
      "0         LeBron James       lebron james\n",
      "1           Chris Paul         chris paul\n",
      "2           Kyle Lowry         kyle lowry\n",
      "3          P.J. Tucker          pj tucker\n",
      "4         Kevin Durant       kevin durant\n",
      "5           Al Horford         al horford\n",
      "6          Mike Conley        mike conley\n",
      "7           Jeff Green         jeff green\n",
      "8    Russell Westbrook  russell westbrook\n",
      "9           Kevin Love         kevin love\n",
      "10         Eric Gordon        eric gordon\n",
      "11         Brook Lopez        brook lopez\n",
      "12       Nicolas Batum      nicolas batum\n",
      "13      DeAndre Jordan     deandre jordan\n",
      "14        James Harden       james harden\n",
      "\n",
      "--- Names from Salary File: ---\n",
      "                     Player_Name           merge_key\n",
      "0              Young  Trae Young          trae young\n",
      "1           Capela  Clint Capela        clint capela\n",
      "2           LeVert  Caris LeVert        caris levert\n",
      "3        Okongwu  Onyeka Okongwu      onyeka okongwu\n",
      "4   Risacher  Zaccharie Risacher  zaccharie risacher\n",
      "5             Mann  Terance Mann        terance mann\n",
      "6     Nance Jr.  Larry Nance Jr.   jr larry nance jr\n",
      "7           Niang  Georges Niang       georges niang\n",
      "8         Daniels  Dyson Daniels       dyson daniels\n",
      "9         Johnson  Jalen Johnson       jalen johnson\n",
      "10           Bufkin  Kobe Bufkin         kobe bufkin\n",
      "11     Mathews  Garrison Mathews    garrison mathews\n",
      "12            Krejci  Vit Krejci          vit krejci\n",
      "13         Gueye  Mouhamed Gueye      mouhamed gueye\n",
      "14       Barlow  Dominick Barlow     dominick barlow\n",
      "--- End Debug ---\n",
      "Player Base rows: 569 | Salary rows: 450\n",
      "Merge complete. Result rows: 409\n",
      "‚ö†Ô∏è Players dropped (no salary match): 160\n",
      "\n",
      "--- [Step 4] Merging Player Popularity (on Standardized Name) ---\n",
      "Player Master rows: 409 | Popularity rows: 511\n",
      "Left merge complete. Result rows: 409\n",
      "Players kept (from left merge): 409\n",
      "\n",
      "--- [Step 5] Cleaning and Saving Player Dataset ---\n",
      "\n",
      "‚úÖ --- PLAYER MERGE COMPLETE --- ‚úÖ\n",
      "Final player dataset with 409 rows saved to:\n",
      "C:\\Users\\tyler\\School\\Learn Statistics\\STA 160\\Project\\notebooks\\Tyler\\merged_player_data.csv\n",
      "\n",
      "--- NEXT STEP ---\n",
      "We now need to merge the team data (Caps, Owners, Stadiums).\n",
      "This will require creating a mapping between 'TEAM_ID', 'team' (abbreviation), and 'Team Name'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# --- File Configuration ---\n",
    "BASE_PATH = '../../data/raw/' # Set the base path for all files\n",
    "Path(BASE_PATH).mkdir(parents=True, exist_ok=True) # Ensure data/raw directory exists just in case\n",
    "\n",
    "# Source files (Originals)\n",
    "STATS_FILE = os.path.join(BASE_PATH, 'Player_Performance_raw.csv')\n",
    "CONTEXT_FILE = os.path.join(BASE_PATH, 'raw_player_context.csv')\n",
    "SALARY_FILE = os.path.join(BASE_PATH, 'raw_player_salaries.csv')\n",
    "POPULARITY_FILE = os.path.join(BASE_PATH, 'nba_player_popularity.csv')\n",
    "\n",
    "# --- NOTE: Team files will be handled in a separate step ---\n",
    "# CAPS_FILE = os.path.join(BASE_PATH, 'raw_salary_caps.csv')\n",
    "# OWNERS_FILE = os.path.join(BASE_PATH, 'Owner Net Worth in Billions .csv')\n",
    "# STADIUMS_FILE = os.path.join(BASE_PATH, 'nba_stadiums.csv')\n",
    "\n",
    "# Final output file\n",
    "OUTPUT_FILE = 'merged_player_data.csv' # Output for this script\n",
    "\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "def standardize_player_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans and standardizes player names so merges work across data sources.\n",
    "    (Copied from your cleaning_helpers.py)\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize unicode (e.g., accents)\n",
    "    try:\n",
    "        # Handle potential empty strings or NaNs that become float\n",
    "        name = str(name)\n",
    "        name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not normalize name '{name}'. Error: {e}\")\n",
    "        pass # Continue with the original name if normalization fails\n",
    "\n",
    "    # Replace punctuation and collapse spaces\n",
    "    name = re.sub(r'[^a-zA-Z\\s]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    \n",
    "    return name.strip().lower()\n",
    "\n",
    "\n",
    "def load_data(filename: str, required_cols: list = None, dtype: dict = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a CSV file with robust debugging and error checking.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to load file: '{filename}'...\")\n",
    "    \n",
    "    # 1. Check if file exists\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"--- üî¥ ERROR: File not found! ---\")\n",
    "        print(f\"Script stopped. Could not find file: {filename}\")\n",
    "        sys.exit(1) # Stop the script\n",
    "        \n",
    "    # 2. Load the data\n",
    "    try:\n",
    "        # Use skipinitialspace=True to handle spaces in column names from output\n",
    "        df = pd.read_csv(filename, skipinitialspace=True, dtype=dtype)\n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        print(f\"‚úÖ Successfully loaded '{filename}'. Found {len(df)} rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"--- üî¥ ERROR: Could not read file! ---\")\n",
    "        print(f\"Could not load {filename}. Error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 3. Check for required columns\n",
    "    if required_cols:\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"--- üî¥ ERROR: Missing required columns in '{filename}'! ---\")\n",
    "            print(f\"Expected columns: {required_cols}\")\n",
    "            print(f\"Missing columns: {missing_cols}\")\n",
    "            print(f\"All columns found: {list(df.columns)}\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def clean_salary_player_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the proper 'First Last' name from the salary file's\n",
    "    'Last First Last' format (e.g., \"Young Trae Young\").\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Split ONLY on the first space\n",
    "        # \"Young Trae Young\" -> [\"Young\", \"Trae Young\"]\n",
    "        # \"Nance Jr. Larry Nance Jr.\" -> [\"Nance\", \"Jr. Larry Nance Jr.\"]\n",
    "        # \"Dick Gradey Dick\" -> [\"Dick\", \"Gradey Dick\"]\n",
    "        parts = name.split(' ', 1)\n",
    "        \n",
    "        if len(parts) == 2:\n",
    "            # Return the second part, which is the \"Firstname Lastname\"\n",
    "            return parts[1]\n",
    "        else:\n",
    "            # If there's no space, just return the name as-is\n",
    "            return name\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not clean salary name '{name}'. Error: {e}\")\n",
    "        return name\n",
    "\n",
    "def clean_team_id(df: pd.DataFrame, col_name='TEAM_ID') -> pd.DataFrame:\n",
    "    \"\"\"Converts TEAM_ID column to a standardized integer format for merging.\"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        print(f\"--- üî¥ ERROR: Tried to clean '{col_name}' but column does not exist.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "        # Drop rows where TEAM_ID could not be converted (became <NA>)\n",
    "        rows_before = len(df)\n",
    "        df = df.dropna(subset=[col_name])\n",
    "        rows_after = len(df)\n",
    "        if rows_before > rows_after:\n",
    "            print(f\"‚ö†Ô∏è Dropped {rows_before - rows_after} rows with invalid/missing TEAM_ID.\")\n",
    "        \n",
    "        df[col_name] = df[col_name].astype('Int64')\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"--- üî¥ ERROR: Could not convert '{col_name}' to a number for merging. ---\")\n",
    "        print(f\"Error: {e}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to merge all PLAYER-related data.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Player Merge Process ---\")\n",
    "\n",
    "    # --- Part 1: Load Player Data ---\n",
    "    print(\"\\n--- [Step 1] Loading Player Data ---\")\n",
    "    # Note: Using the column names from your output\n",
    "    df_stats = load_data(STATS_FILE, ['PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID'])\n",
    "    df_context = load_data(CONTEXT_FILE, ['PLAYER_ID', 'PLAYER_NAME', 'BIRTHDATE'])\n",
    "    df_salary = load_data(SALARY_FILE, ['Player_Name', 'Salary'])\n",
    "    df_popularity = load_data(POPULARITY_FILE, ['Player', 'Followers'])\n",
    "\n",
    "    # --- Part 2: Merge Stats + Context ---\n",
    "    print(\"\\n--- [Step 2] Merging Stats and Context (on PLAYER_ID) ---\")\n",
    "    rows_stats = len(df_stats)\n",
    "    rows_context = len(df_context)\n",
    "    \n",
    "    # Using 'inner' merge as requested\n",
    "    df_player_base = pd.merge(\n",
    "        df_stats, \n",
    "        df_context, \n",
    "        on=\"PLAYER_ID\", \n",
    "        how=\"inner\",\n",
    "        suffixes=('_stats', '_context')\n",
    "    )\n",
    "    \n",
    "    print(f\"Stats rows: {rows_stats} | Context rows: {rows_context}\")\n",
    "    print(f\"Merge complete. Result rows: {len(df_player_base)}\")\n",
    "    \n",
    "    if len(df_player_base) == 0:\n",
    "        print(\"--- üî¥ ERROR: Merge 1 (Stats + Context) resulted in 0 rows. ---\")\n",
    "        print(\"Please check PLAYER_ID columns in both files.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # --- Part 3: Merge Salaries ---\n",
    "    print(\"\\n--- [Step 3] Merging Salaries (on Standardized Name) ---\")\n",
    "    \n",
    "    # Standardize names for merging\n",
    "    # Note: Use the PLAYER_NAME from the context file if it exists and is cleaner\n",
    "    df_player_base['merge_key'] = df_player_base['PLAYER_NAME_context'].apply(standardize_player_name)\n",
    "    \n",
    "    # --- THIS IS THE FIX ---\n",
    "    # 1. Clean the salary name structure (e.g., \"Young Trae Young\" -> \"Trae Young\")\n",
    "    # 2. Standardize the *cleaned* name (e.g., \"Trae Young\" -> \"trae young\")\n",
    "    df_salary['merge_key'] = df_salary['Player_Name'].apply(clean_salary_player_name).apply(standardize_player_name)\n",
    "    # --- END FIX ---\n",
    "    \n",
    "    # --- DEBUGGING PRINT STATEMENTS ---\n",
    "    print(\"\\n--- DEBUG: Checking standardized names for merge ---\")\n",
    "    print(\"\\n--- Names from Player Base (Stats/Context): ---\")\n",
    "    print(df_player_base[['PLAYER_NAME_context', 'merge_key']].head(15).to_string())\n",
    "    \n",
    "    print(\"\\n--- Names from Salary File: ---\")\n",
    "    print(df_salary[['Player_Name', 'merge_key']].head(15).to_string())\n",
    "    print(\"--- End Debug ---\")\n",
    "    # --- END DEBUGGING ---\n",
    "    \n",
    "    rows_before_salary = len(df_player_base)\n",
    "    rows_salary = len(df_salary)\n",
    "    \n",
    "    # Using 'inner' merge as requested in your script\n",
    "    # We can change to 'left' if you want to keep players without salary info\n",
    "    df_player_master = pd.merge(\n",
    "        df_player_base, \n",
    "        df_salary, \n",
    "        on=\"merge_key\", \n",
    "        how=\"inner\" # Change to \"left\" to keep all players\n",
    "    )\n",
    "    \n",
    "    print(f\"Player Base rows: {rows_before_salary} | Salary rows: {rows_salary}\")\n",
    "    print(f\"Merge complete. Result rows: {len(df_player_master)}\")\n",
    "    print(f\"‚ö†Ô∏è Players dropped (no salary match): {rows_before_salary - len(df_player_master)}\")\n",
    "    \n",
    "    if len(df_player_master) == 0:\n",
    "        print(\"--- üî¥ ERROR: Merge 2 (Salaries) resulted in 0 rows. ---\")\n",
    "        print(\"Check name standardization or if salary file is correct.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # --- Part 4: Merge Player Popularity (NEW) ---\n",
    "    print(\"\\n--- [Step 4] Merging Player Popularity (on Standardized Name) ---\")\n",
    "    df_popularity['merge_key'] = df_popularity['Player'].apply(standardize_player_name)\n",
    "    \n",
    "    rows_before_pop = len(df_player_master)\n",
    "    rows_pop = len(df_popularity)\n",
    "    \n",
    "    # Use LEFT merge to keep all players, even if not on popularity list\n",
    "    df_player_master = pd.merge(\n",
    "        df_player_master,\n",
    "        df_popularity.drop(columns=['Player'], errors='ignore'), # Drop original name col\n",
    "        on=\"merge_key\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Player Master rows: {rows_before_pop} | Popularity rows: {rows_pop}\")\n",
    "    print(f\"Left merge complete. Result rows: {len(df_player_master)}\")\n",
    "    print(f\"Players kept (from left merge): {len(df_player_master)}\")\n",
    "\n",
    "    # --- Part 5: Final Cleanup and Save ---\n",
    "    print(\"\\n--- [Step 5] Cleaning and Saving Player Dataset ---\")\n",
    "    \n",
    "    # Clean up salary/cap/numeric columns\n",
    "    numeric_cols_to_clean = [\n",
    "        'Salary', 'Followers'\n",
    "    ]\n",
    "    \n",
    "    for col in numeric_cols_to_clean:\n",
    "        if col in df_player_master.columns:\n",
    "            df_player_master[col] = (\n",
    "                df_player_master[col]\n",
    "                .astype(str)\n",
    "                .str.replace(r\"[^\\d.]\", \"\", regex=True) # Keep digits and decimals\n",
    "                .replace(\"\", None)\n",
    "                .astype(float)\n",
    "            )\n",
    "    \n",
    "    # Save to output file\n",
    "    output_path = Path(OUTPUT_FILE)\n",
    "    df_player_master.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ --- PLAYER MERGE COMPLETE --- ‚úÖ\")\n",
    "    print(f\"Final player dataset with {len(df_player_master)} rows saved to:\")\n",
    "    print(f\"{output_path.resolve()}\")\n",
    "    \n",
    "    print(\"\\n--- NEXT STEP ---\")\n",
    "    print(\"We now need to merge the team data (Caps, Owners, Stadiums).\")\n",
    "    print(\"This will require creating a mapping between 'TEAM_ID', 'team' (abbreviation), and 'Team Name'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0eb10a",
   "metadata": {},
   "source": [
    "# now with team ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acd2f1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Full Merge Process ---\n",
      "\n",
      "--- [Step 1] Loading Player Data ---\n",
      "Attempting to load file: '../../data/raw/Player_Performance_raw.csv'...\n",
      "‚úÖ Successfully loaded '../../data/raw/Player_Performance_raw.csv'. Found 569 rows.\n",
      "Attempting to load file: '../../data/raw/raw_player_context.csv'...\n",
      "‚úÖ Successfully loaded '../../data/raw/raw_player_context.csv'. Found 569 rows.\n",
      "Attempting to load file: '../../data/raw/raw_player_salaries.csv'...\n",
      "‚úÖ Successfully loaded '../../data/raw/raw_player_salaries.csv'. Found 450 rows.\n",
      "Attempting to load file: '../../data/raw/nba_player_popularity.csv'...\n",
      "‚úÖ Successfully loaded '../../data/raw/nba_player_popularity.csv'. Found 511 rows.\n",
      "\n",
      "--- [Step 2] Building Player Master List ---\n",
      "Merging Stats + Context...\n",
      "Merging Salaries...\n",
      "Merge complete. Result rows: 409\n",
      "‚ö†Ô∏è Players dropped (no salary match): 160\n",
      "Merging Player Popularity...\n",
      "Player Master list complete. Total rows: 409\n",
      "\n",
      "--- [Step 3] Loading Team Data ---\n",
      "Attempting to load file: '../../data/raw/raw_salary_caps.csv'...\n",
      "‚úÖ Successfully loaded '../../data/raw/raw_salary_caps.csv'. Found 30 rows.\n",
      "Attempting to load file: '../../data/raw/Owner Net Worth in Billions .csv'...\n",
      "‚úÖ Successfully loaded '../../data/raw/Owner Net Worth in Billions .csv'. Found 32 rows.\n",
      "Attempting to load file: '../../data/raw/nba_stadiums.csv'...\n",
      "‚úÖ Successfully loaded '../../data/raw/nba_stadiums.csv'. Found 30 rows.\n",
      "\n",
      "--- [Step 4] Building Team Master List ---\n",
      "‚ö†Ô∏è Dropped 2 rows with invalid/missing TEAM_ID in column 'TEAM_ID'.\n",
      "Merging Caps + Owners...\n",
      "Team Base (Caps+Owners) complete. Result rows: 29\n",
      "Merging Stadium Data...\n",
      "Team Master list complete. Total rows: 29\n",
      "\n",
      "--- [Step 5] Final Merge: Players + Teams (on TEAM_ID) ---\n",
      "Player Master rows: 409 | Team Master rows: 29\n",
      "Final merge complete. Final rows: 409\n",
      "\n",
      "--- [Step 6] Cleaning and Saving Final Dataset ---\n",
      "\n",
      "‚úÖ --- PROCESS COMPLETE --- ‚úÖ\n",
      "Final dataset with 409 rows and 86 columns saved to:\n",
      "C:\\Users\\tyler\\School\\Learn Statistics\\STA 160\\Project\\notebooks\\Tyler\\master_dataset_v3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Temp\\ipykernel_30616\\2160656193.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col_name] = df[col_name].astype('Int64')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# --- File Configuration ---\n",
    "BASE_PATH = '../../data/raw/' # Set the base path for all files\n",
    "\n",
    "# Player files\n",
    "STATS_FILE = os.path.join(BASE_PATH, 'Player_Performance_raw.csv')\n",
    "CONTEXT_FILE = os.path.join(BASE_PATH, 'raw_player_context.csv')\n",
    "SALARY_FILE = os.path.join(BASE_PATH, 'raw_player_salaries.csv')\n",
    "POPULARITY_FILE = os.path.join(BASE_PATH, 'nba_player_popularity.csv')\n",
    "\n",
    "# Team files\n",
    "CAPS_FILE = os.path.join(BASE_PATH, 'raw_salary_caps.csv')\n",
    "STADIUMS_FILE = os.path.join(BASE_PATH, 'nba_stadiums.csv')\n",
    "OWNERS_FILE = os.path.join(BASE_PATH, 'Owner Net Worth in Billions .csv')\n",
    "\n",
    "# Final output file\n",
    "OUTPUT_FILE = 'master_dataset_v3.csv'\n",
    "\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "def standardize_player_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans and standardizes player names so merges work across data sources.\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize unicode (e.g., accents)\n",
    "    try:\n",
    "        name = str(name)\n",
    "        name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not normalize name '{name}'. Error: {e}\")\n",
    "        pass\n",
    "\n",
    "    # Replace punctuation and collapse spaces\n",
    "    name = re.sub(r'[^a-zA-Z\\s]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    \n",
    "    return name.strip().lower()\n",
    "\n",
    "\n",
    "def load_data(filename: str, required_cols: list = None, dtype: dict = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a CSV file with robust debugging and error checking.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to load file: '{filename}'...\")\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"--- üî¥ ERROR: File not found! ---\")\n",
    "        print(f\"Script stopped. Could not find file: {filename}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    try:\n",
    "        # Use skipinitialspace=True to handle spaces in column names\n",
    "        df = pd.read_csv(filename, skipinitialspace=True, dtype=dtype)\n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        print(f\"‚úÖ Successfully loaded '{filename}'. Found {len(df)} rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"--- üî¥ ERROR: Could not read file! ---\")\n",
    "        print(f\"Could not load {filename}. Error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if required_cols:\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"--- üî¥ ERROR: Missing required columns in '{filename}'! ---\")\n",
    "            print(f\"Expected columns: {required_cols}\")\n",
    "            print(f\"Missing columns: {missing_cols}\")\n",
    "            print(f\"All columns found: {list(df.columns)}\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def clean_salary_player_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the proper 'First Last' name from the salary file's\n",
    "    'Last First Last' format (e.g., \"Young Trae Young\").\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Split ONLY on the first space\n",
    "        parts = name.split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            return parts[1] # Return the \"Firstname Lastname\" part\n",
    "        else:\n",
    "            return name\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not clean salary name '{name}'. Error: {e}\")\n",
    "        return name\n",
    "\n",
    "def clean_team_id(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Converts a specified TEAM_ID column to a standardized integer format.\"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        print(f\"--- üî¥ ERROR: Tried to clean '{col_name}' but column does not exist.\")\n",
    "        print(f\"All columns found: {list(df.columns)}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "        rows_before = len(df)\n",
    "        df = df.dropna(subset=[col_name])\n",
    "        rows_after = len(df)\n",
    "        if rows_before > rows_after:\n",
    "            print(f\"‚ö†Ô∏è Dropped {rows_before - rows_after} rows with invalid/missing TEAM_ID in column '{col_name}'.\")\n",
    "        \n",
    "        df[col_name] = df[col_name].astype('Int64')\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"--- üî¥ ERROR: Could not convert '{col_name}' to a number for merging. ---\")\n",
    "        print(f\"Error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def clean_numeric_cols(df: pd.DataFrame, cols_to_clean: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans specified columns by removing non-numeric characters\n",
    "    and converting them to floats.\n",
    "    \"\"\"\n",
    "    for col in cols_to_clean:\n",
    "        if col in df.columns:\n",
    "            df[col] = (\n",
    "                df[col]\n",
    "                .astype(str)\n",
    "                .str.replace(r\"[^\\d.]\", \"\", regex=True) # Keep digits and decimals\n",
    "                .replace(\"\", None)\n",
    "                .astype(float)\n",
    "            )\n",
    "    return df\n",
    "        \n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the entire merge workflow.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Full Merge Process ---\")\n",
    "\n",
    "    # --- Part 1: Load Player Data ---\n",
    "    print(\"\\n--- [Step 1] Loading Player Data ---\")\n",
    "    df_stats = load_data(STATS_FILE, ['PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID'])\n",
    "    df_context = load_data(CONTEXT_FILE, ['PLAYER_ID', 'PLAYER_NAME'])\n",
    "    df_salary = load_data(SALARY_FILE, ['Player_Name', 'Salary'])\n",
    "    df_popularity = load_data(POPULARITY_FILE, ['Player', 'Followers'])\n",
    "\n",
    "    # --- Part 2: Build Player Master DataFrame ---\n",
    "    print(\"\\n--- [Step 2] Building Player Master List ---\")\n",
    "    \n",
    "    # Merge Stats + Context (on PLAYER_ID)\n",
    "    print(\"Merging Stats + Context...\")\n",
    "    df_player_master = pd.merge(\n",
    "        df_stats, \n",
    "        df_context, \n",
    "        on=\"PLAYER_ID\", \n",
    "        how=\"inner\",\n",
    "        suffixes=('_stats', '_context')\n",
    "    )\n",
    "    \n",
    "    # Merge Salaries (on Standardized Name)\n",
    "    print(\"Merging Salaries...\")\n",
    "    df_player_master['merge_key'] = df_player_master['PLAYER_NAME_context'].apply(standardize_player_name)\n",
    "    df_salary['merge_key'] = df_salary['Player_Name'].apply(clean_salary_player_name).apply(standardize_player_name)\n",
    "    \n",
    "    rows_before_salary = len(df_player_master)\n",
    "    df_player_master = pd.merge(\n",
    "        df_player_master, \n",
    "        df_salary, \n",
    "        on=\"merge_key\", \n",
    "        how=\"inner\" # Keep only players with salary info\n",
    "    )\n",
    "    print(f\"Merge complete. Result rows: {len(df_player_master)}\")\n",
    "    print(f\"‚ö†Ô∏è Players dropped (no salary match): {rows_before_salary - len(df_player_master)}\")\n",
    "\n",
    "    # Merge Player Popularity (on Standardized Name)\n",
    "    print(\"Merging Player Popularity...\")\n",
    "    df_popularity['merge_key'] = df_popularity['Player'].apply(standardize_player_name)\n",
    "    \n",
    "    df_player_master = pd.merge(\n",
    "        df_player_master,\n",
    "        df_popularity.drop(columns=['Player'], errors='ignore'),\n",
    "        on=\"merge_key\",\n",
    "        how=\"left\" # Keep all players, even if no popularity info\n",
    "    )\n",
    "    print(f\"Player Master list complete. Total rows: {len(df_player_master)}\")\n",
    "\n",
    "    # --- Part 3: Load Team Data ---\n",
    "    print(\"\\n--- [Step 3] Loading Team Data ---\")\n",
    "    df_caps = load_data(CAPS_FILE, ['team_id', 'team'])\n",
    "    df_owners = load_data(OWNERS_FILE, ['Team ID', 'Team Name'])\n",
    "    df_stadiums = load_data(STADIUMS_FILE, ['TEAM_ABBREVIATION', 'Stadium_Name'])\n",
    "\n",
    "    # --- Part 4: Build Team Master DataFrame ---\n",
    "    print(\"\\n--- [Step 4] Building Team Master List ---\")\n",
    "    \n",
    "    # Standardize TEAM_ID column names and types BEFORE merging\n",
    "    df_caps = df_caps.rename(columns={'team_id': 'TEAM_ID'})\n",
    "    # FIX: Drop duplicates to ensure only one row per team\n",
    "    df_caps = clean_team_id(df_caps, 'TEAM_ID').drop_duplicates(subset=['TEAM_ID'])\n",
    "    \n",
    "    df_owners = df_owners.rename(columns={'Team ID': 'TEAM_ID'})\n",
    "    # FIX: Drop duplicates to ensure only one row per team\n",
    "    df_owners = clean_team_id(df_owners, 'TEAM_ID').drop_duplicates(subset=['TEAM_ID'])\n",
    "    \n",
    "    # Drop junk column\n",
    "    if 'Unnamed: 3' in df_owners.columns:\n",
    "        df_owners = df_owners.drop(columns=['Unnamed: 3'])\n",
    "\n",
    "    # Merge Caps + Owners (on TEAM_ID)\n",
    "    print(\"Merging Caps + Owners...\")\n",
    "    df_team_master = pd.merge(\n",
    "        df_caps,\n",
    "        df_owners,\n",
    "        on=\"TEAM_ID\",\n",
    "        how=\"inner\" # Use 'inner' to keep only teams present in both\n",
    "    )\n",
    "    print(f\"Team Base (Caps+Owners) complete. Result rows: {len(df_team_master)}\")\n",
    "\n",
    "    # Merge Stadiums (on Abbreviation)\n",
    "    print(\"Merging Stadium Data...\")\n",
    "    df_team_master = pd.merge(\n",
    "        df_team_master,\n",
    "        df_stadiums,\n",
    "        left_on=\"team\", # Abbreviation from raw_salary_caps.csv\n",
    "        right_on=\"TEAM_ABBREVIATION\",\n",
    "        how=\"left\" # Keep all teams, even if no stadium info\n",
    "    )\n",
    "    print(f\"Team Master list complete. Total rows: {len(df_team_master)}\")\n",
    "\n",
    "    # --- Part 5: Final Merge: Players + Teams ---\n",
    "    print(\"\\n--- [Step 5] Final Merge: Players + Teams (on TEAM_ID) ---\")\n",
    "    \n",
    "    # FIX: Use .copy() to prevent SettingWithCopyWarning\n",
    "    df_player_master_clean = clean_team_id(df_player_master.copy(), 'TEAM_ID')\n",
    "    \n",
    "    rows_players = len(df_player_master_clean)\n",
    "    rows_teams = len(df_team_master)\n",
    "    \n",
    "    df_final = pd.merge(\n",
    "        df_player_master_clean,\n",
    "        df_team_master,\n",
    "        on=\"TEAM_ID\",\n",
    "        how=\"left\" # Keep ALL players, attach team info\n",
    "    )\n",
    "    \n",
    "    print(f\"Player Master rows: {rows_players} | Team Master rows: {rows_teams}\")\n",
    "    print(f\"Final merge complete. Final rows: {len(df_final)}\")\n",
    "    \n",
    "    if len(df_final) != rows_players:\n",
    "        print(f\"‚ö†Ô∏è Warning: Row count changed. Investigate merge. Expected {rows_players}.\")\n",
    "\n",
    "    # --- Part 6: Final Cleanup and Save ---\n",
    "    print(\"\\n--- [Step 6] Cleaning and Saving Final Dataset ---\")\n",
    "    \n",
    "    # Rename messy 'Owner Net Worth' column\n",
    "    if 'Owner Net Worth in Billions' in df_final.columns:\n",
    "        df_final = df_final.rename(columns={\n",
    "            'Owner Net Worth in Billions': 'Owner_Net_Worth_Billions'\n",
    "        })\n",
    "    \n",
    "    # List of ALL numeric columns that need cleaning\n",
    "    all_numeric_cols = [\n",
    "        'Salary', 'Followers', 'total_cap_used', 'remaining_cap_space', \n",
    "        'active_cap', 'active_top_3', 'dead_cap', 'Capacity', \n",
    "        'Construction_Cost', 'Owner_Net_Worth_Billions'\n",
    "    ]\n",
    "    \n",
    "    df_final = clean_numeric_cols(df_final, all_numeric_cols)\n",
    "    \n",
    "    # Save to output file\n",
    "    output_path = Path(OUTPUT_FILE)\n",
    "    df_final.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ --- PROCESS COMPLETE --- ‚úÖ\")\n",
    "    print(f\"Final dataset with {len(df_final)} rows and {len(df_final.columns)} columns saved to:\")\n",
    "    print(f\"{output_path.resolve()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee5f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
