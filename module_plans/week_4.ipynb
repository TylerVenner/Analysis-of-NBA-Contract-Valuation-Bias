{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8b54ac",
   "metadata": {},
   "source": [
    "# Module Contract: `configs/project_config.py`\n",
    "\n",
    "This is the **most important file** for our Week 4 workflow. It is the central \"contract\" and \"single source of truth\" that all our modules will read from.\n",
    "\n",
    "### Why do we need this?\n",
    "\n",
    "1.  **No \"Magic Strings\":** We will never hard-code a file path (e.g., `\"data/raw/my_data.csv\"`) or a constant (e.g., `SEASON = \"2024-25\"`) inside any of our scripts. This is a bad practice that makes code hard to maintain. Instead, every script will import these variables from this one file.\n",
    "2.  **Maintainability:** If we need to change a filename or run the *entire* project for a different season (e.g., \"2023-24\"), we only have to change it **in this one file**.\n",
    "\n",
    "### How does this help us work in parallel?\n",
    "\n",
    "* Modules **do not talk to each other directly**. `get_nba_stats.py` will *never* import `get_salary_data.py`.\n",
    "* Instead, they all import `project_config.py`.\n",
    "* This file tells each module **what inputs to read** and **where to write its output**. For example, Module 2 knows its job is to create `RAW_SALARY_FILE`, and Module 3 knows its job is to read `RAW_SALARY_FILE`. This \"contract\" is all they need to know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd98fd7",
   "metadata": {},
   "source": [
    "## Module 1a: NBA Performance Stats\n",
    "\n",
    "* **Owner:** Gary\n",
    "* **File:** `src/data_collection/get_nba_stats.py`\n",
    "* **Job:** The goal of this module is to fetch all *performance* statistics for every player in the target season. This dataset will become our **$X$ vector** (the predictors) for our clustering and regression models. You'll primarily use the `nba_api` library, likely the `leaguedashplayerstats` endpoint, to get advanced metrics (like `TS_PCT`, `AST_PCT`, etc.) rather than just basic \"per game\" stats.\n",
    "\n",
    "### Inputs (from `project_config.py`):\n",
    "This script will import variables from our central config file.\n",
    "* `SEASON`: The season string (e.g., \"2024-25\") that will be passed directly to the `nba_api` function call.\n",
    "* `RAW_STATS_FILE`: The full `pathlib.Path` object where the final, raw dataframe will be saved as a CSV file (e.g., `.../data/raw/raw_player_stats.csv`).\n",
    "\n",
    "### Output:\n",
    "* **File:** `data/raw/raw_player_stats.csv`\n",
    "* **Description:** This CSV is the \"raw\" output from the API. It should be one row per player.\n",
    "* **Required Columns:**\n",
    "    * `PLAYER_ID`: This is the **most important column** as it's the unique primary key we'll use for the first, easy merge with Module 1b.\n",
    "    * `PLAYER_NAME`: The player's full name as it appears in the API.\n",
    "    * `TEAM_ID`: The unique ID for the player's team.\n",
    "    * **$X$ Vector Stats:** A wide set of advanced stats. We need *at least*:\n",
    "        * `TS_PCT` (True Shooting %)\n",
    "        * `AST_PCT` (Assist %)\n",
    "        * `REB_PCT` (Rebound %)\n",
    "        * `USG_PCT` (Usage %)\n",
    "        * `BLK_PCT` (Block %)\n",
    "        * `STL_PCT` (Steal %)\n",
    "        * `FGA_2PT_PCT` (Percentage of shots that are 2-pointers)\n",
    "        * `FGA_3PT_PCT` (Percentage of shots that are 3-pointers)\n",
    "        * ...and any other rate-based stats we find useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b808280",
   "metadata": {},
   "source": [
    "## Module 1b: NBA Contextual Data\n",
    "\n",
    "* **Owner:** Alberto\n",
    "* **File:** `src/data_collection/get_context_data.py`\n",
    "* **Job:** This module's job is to fetch the *non-performance*, demographic, and career-related data for all players. This will become our $\\mathbf{Z}_{\\text{context}}$ vector, which is the key to our final bias analysis (Phase 4). We need to get this data separately because it often comes from a different API endpoint (like `commonplayerinfo`) than the advanced stats (which come from `leaguedashplayerstats`).\n",
    "\n",
    "### Inputs (from `project_config.py`):\n",
    "This script will import variables from our central config file.\n",
    "* `SEASON`: This will be used to get the list of active players for that season. The script will likely need to get a list of all player IDs first, then loop through them to call the `commonplayerinfo` endpoint for each one.\n",
    "* `RAW_CONTEXT_FILE`: The full `pathlib.Path` object where the final, raw dataframe will be saved as a CSV (e.g., `.../data/raw/raw_player_context.csv`).\n",
    "\n",
    "### Output (The Contract):\n",
    "* **File:** `data/raw/raw_player_context.csv`\n",
    "* **Description:** This CSV will be one row per player, containing their \"biographical\" data.\n",
    "* **Required Columns:**\n",
    "    * `PLAYER_ID`: The **primary key**. This is critical as it's how this file will be merged with `raw_player_stats.csv` in Module 3.\n",
    "    * `PLAYER_NAME`: The player's full name from the API (for cross-referencing).\n",
    "    * `BIRTHDATE`: The player's date of birth (e.g., \"1984-12-30T00:00:00\"). We **must** have this so we can calculate their `AGE` during the processing step.\n",
    "    * `COUNTRY`: The player's home country (for our 'Nationality' bias test).\n",
    "    * `DRAFT_YEAR`: The year the player was drafted.\n",
    "    * `DRAFT_ROUND`: The round the player was drafted in.\n",
    "    * `DRAFT_NUMBER`: The overall draft pick number. (These three draft columns are crucial for our \"pedigree\" bias test)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae5969",
   "metadata": {},
   "source": [
    "## Module 2: Salary Data Scraping\n",
    "\n",
    "* **Owner:** Tyler and Macy\n",
    "* **File:** `src/data_collection/get_salary_data.py`\n",
    "* **Job:** This is one of the most critical and difficult modules. Its only job is to go to an external website (like **Spotrac** or **Basketball-Reference**), scrape the salary table for our target season, and save that raw data. This module is completely separate from the `nba_api` because it's a \"messy\" data source. The player names *will not* match the API, and the salary figures will be text (e.g., \"$15,000,000\") that needs cleaning. This module's job is *not* to clean the data, just to get it and save it.\n",
    "\n",
    "### Inputs (from `project_config.py`):\n",
    "This script will import variables from our central config file.\n",
    "* `SEASON`: The season string (e.g., \"2024-25\") will be used to construct the correct URL to scrape (e.g., `https://spotrac.com/nba/payroll/2024/`).\n",
    "* `RAW_SALARY_FILE`: The full `pathlib.Path` object where the final, raw dataframe will be saved as a CSV (e.g., `.../data/raw/raw_player_salaries.csv`).\n",
    "\n",
    "### Output (The Contract):\n",
    "* **File:** `data/raw/raw_player_salaries.csv`\n",
    "* **Description:** This CSV is the raw, uncleaned table scraped from the website.\n",
    "* **Required Columns:**\n",
    "    * `Player_Name`: The player's name exactly as it appears on the website (e.g., \"LucMbah a Moute\"). This will be our messy key for the merge.\n",
    "    * `Salary`: The player's salary as a *string* (e.g., \"$15,000,000\", or \"$1,200,000 (cap)\") exactly as it appears on the site. We will clean this in Module 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf31fa64",
   "metadata": {},
   "source": [
    "## Module 3: Merging & Cleaning\n",
    "\n",
    "* **Owners:** Leo\n",
    "* **File:** `src/data_processing/merge_data.py`\n",
    "* **Job:** This is the most important and complex module of Week 4. It's the \"integration\" step where all our raw data comes together. Its job is to take the three separate, raw files (stats, context, and salary), combine them into one, and \"process\" them into a final, clean master file. This module handles all the dirty work: merging on different keys, cleaning messy text data, and calculating new variables.\n",
    "\n",
    "### Helper Script: `src/data_processing/cleaning_helpers.py`\n",
    "* This module's logic will be complex, so we'll put our reusable cleaning functions in a separate file.\n",
    "* It **MUST** contain a function: `standardize_player_name(name: str) -> str`.\n",
    "* **Explanation:** This function is our \"Rosetta Stone.\" It will take a messy name from any source (e.g., \"LucMbah a Moute\", \"LeBron James.\", \"Luka Dončić\") and convert it to a single, standardized key (e.g., \"luc mbah a moute\", \"lebron james\", \"luka doncic\"). We will apply this function to the name columns from *both* the NBA API data and the salary data before attempting the merge. This is how we solve the \"different names\" problem.\n",
    "\n",
    "### Inputs (The Contract):\n",
    "This script will read three files, using the paths from `project_config.py`:\n",
    "* `RAW_STATS_FILE` (from Module 1a)\n",
    "* `RAW_CONTEXT_FILE` (from Module 1b)\n",
    "* `RAW_SALARY_FILE` (from Module 2)\n",
    "\n",
    "### Logic:\n",
    "1.  **Load Stats & Context:** Load `raw_player_stats.csv` and `raw_player_context.csv`.\n",
    "2.  **First Merge (Easy):** Perform an inner merge on these two dataframes using `PLAYER_ID` as the key. This gives us one unified \"NBA API\" dataframe.\n",
    "3.  **Load Salaries:** Load `raw_player_salaries.csv`.\n",
    "4.  **Standardize Keys:**\n",
    "    * Apply `standardize_player_name` to the `PLAYER_NAME` column of the NBA API dataframe, creating a new `merge_key` column.\n",
    "    * Apply `standardize_player_name` to the `Player_Name` column of the salary dataframe, creating its `merge_key` column.\n",
    "5.  **Second Merge (Hard):** Perform a **left merge**, joining the salary data *onto* our main NBA API dataframe using `merge_key`. We use a left merge so we keep all players, even if they are missing salary data.\n",
    "6.  **Clean & Process:**\n",
    "    * **Clean Salary:** Convert the `Salary` column (e.g., \"$15,000,000\") to a clean numeric type (e.g., `15000000`).\n",
    "    * **Calculate Age:** Use the `BIRTHDATE` column to calculate the player's `AGE` at the start of the `SEASON`.\n",
    "    * **Handle NaNs:** Explicitly check for players with `NaN` in the `Salary` column. We will log the names of these \"merge failures\" to the console to see who we missed.\n",
    "7.  **Save:** Save the final, processed dataframe.\n",
    "\n",
    "### Output (The Contract):\n",
    "* **File:** `data/processed/merged_player_data_v1.csv`\n",
    "* **Description:** This is the **final, golden dataset** for the entire team. It should have one row per player, with all performance ($X$), context ($Z_{\\text{context}}$), and salary ($Y$) columns cleaned and in the correct data type. All subsequent modules (EDA, Clustering, Modeling) will read from this *one file*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a0d516",
   "metadata": {},
   "source": [
    "## Module 4: EDA & Validation\n",
    "\n",
    "* **Owners:** -- \n",
    "* **File:** `notebooks/Macy/00_sandbox.ipynb`, `notebooks/Alberto/00_sandbox.ipynb`\n",
    "* **Job:** This module is our \"Quality Assurance\" (QA) step. The owners act as the first \"customers\" or \"users\" of the data produced by Module 3. The goal is twofold: 1) **Validate** that the merged dataset is complete and correct, and 2) **Explore** the data (EDA) to understand its properties, which will inform our future modeling decisions in Phase 1 (Clustering) and Phase 3 (Salary Modeling).\n",
    "\n",
    "### Inputs (The Contract):\n",
    "This module reads *only one file*, the final output from Module 3.\n",
    "* `PROCESSED_MERGED_FILE` (i.e., `data/processed/merged_player_data_v1.csv`), which is imported from `project_config.py`.\n",
    "\n",
    "### Tasks:\n",
    "1.  **Load & Validate:**\n",
    "    * Load `PROCESSED_MERGED_FILE` into a pandas DataFrame.\n",
    "    * Immediately run `df.info()` and `df.describe()`.\n",
    "    * **Critical Validation:** Check the number of non-null values for the `Salary` column. This is our **merge success rate**. (e.g., \"We have 540 players in the NBA stats, but only 480 have non-null salaries. Our merge success rate is 88.8%\"). This is the most important metric to report back to the team.\n",
    "    * Check the data types (`dtypes`). Is `Salary` numeric? Is `AGE` numeric? Are all our $X$ stats numeric?\n",
    "\n",
    "2.  **Initial Analysis (EDA):**\n",
    "    * **Salary Distribution:** Plot a histogram of the `Salary` column. Then, plot a histogram of `log(Salary)`. This is essential to confirm our hypothesis that salary is log-normally distributed and that we **must** use `log(Salary)` as our $Y$ variable in Phase 3.\n",
    "    * **Predictor Distributions:** Plot histograms for 3-4 key variables from our $X$ vector (e.g., `TS_PCT`, `USG_PCT`) and our $Z_{\\text{context}}$ vector (e.g., `AGE`). This helps us spot skewness or outliers *before* we feed them into our clustering algorithm.\n",
    "    * **Correlation:** Create a correlation heatmap of our main $X$ vector variables. This will show us which stats are highly correlated (e.g., `FGA_2PT_PCT` and `FGA_3PT_PCT` will be negatively correlated) and helps us understand the structure of our data.\n",
    "\n",
    "3.  **Report:**\n",
    "    * Post a summary of the findings in the team chat (e.g., \"Data loaded. Merge success rate is 90%. `log(Salary)` looks good. `USG_PCT` is a bit skewed, but nothing major. We are clear to proceed to Phase 1.\").\n",
    "\n",
    "### Parallel Work:\n",
    "* This module can be **started immediately**. The owners can create a *fake, 5-row* `merged_player_data_v1.csv` file with the expected column names. They can write their entire notebook (all the `df.info()`, `df.hist()`, etc. calls) using this fake data. When Module 3 is finally complete, they just re-run their notebook with the *real* data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08b735",
   "metadata": {},
   "source": [
    "## Discussion: How This All Works Together\n",
    "\n",
    "This modular structure is the most important part of our Week 4 plan. It's how we'll get a complex, multi-part task done in one week. Here's why this setup is designed for success:\n",
    "\n",
    "### 1. It Enforces \"Decoupling\"\n",
    "* **What it means:** Notice that `get_nba_stats.py` and `get_salary_data.py` **do not depend on each other at all**. They are \"decoupled.\" They don't import each other's code, and one's failure doesn't stop the other from running.\n",
    "* **Why it's smart:** This allows for **true parallel work**. -- and -- can start their coding the moment they read this plan, without a single check-in. Their only shared dependency is the *filename* they write to, which is defined in our \"contract,\" the `project_config.py` file.\n",
    "\n",
    "### 2. It Creates a \"Consumer\" Model\n",
    "* **What it means:** -- and -- (Module 4) are the \"consumers\" of the data. They don't need to know *how* -- scraped the data or *how* -- merge logic works. They only care about the final, promised output file: `merged_player_data_v1.csv`.\n",
    "* **Why it's smart:** This **also enables parallel work**. -- and -- don't have to wait for Module 3 to be finished. They can **create a fake, 5-row \"dummy\" CSV** with the exact column names we've specified in this plan. With that dummy file, they can build their entire EDA notebook—all the plots, all the validation checks (`df.info()`, etc.). When Module 3 is finally done, they just re-run their *already-finished* notebook with the *real* data.\n",
    "\n",
    "### 3. It Makes Testing and Debugging 10x Easier\n",
    "* **What it means:** Because each part is a separate script, we can run just one part at a time.\n",
    "* **Why it's smart:** If `main.py` fails, we'll know exactly which module is broken. If the salary scraper (Module 2) breaks because the website changed, we can fix it without touching or worrying about the NBA API (Module 1). This isolation is key to managing a complex data pipeline and not wasting time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674d00a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
