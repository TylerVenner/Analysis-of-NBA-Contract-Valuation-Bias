{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18aa4c6",
   "metadata": {},
   "source": [
    "# DML Pipeline - Module Specifications\n",
    "\n",
    "This document outlines the modular functions we need to build to implement the Double-Residual Machine Learning (DML) pipeline.\n",
    "\n",
    "## The Workflow\n",
    "\n",
    "The analysis is broken into three main functional components:\n",
    "\n",
    "1.  **Module 1: `train_f_model`**\n",
    "    -   **Purpose:** Trains the \"Outcome Model\" ($\\hat{f}$).\n",
    "    -   **Model:** $Y \\sim X$ (Salary ~ Performance)\n",
    "    -   **Assigned to:** Leo\n",
    "\n",
    "2.  **Module 2: `train_h_models`**\n",
    "    -   **Purpose:** Trains the \"Treatment Models\" ($\\hat{h}$). We train *one model for each* contextual/bias factor.\n",
    "    -   **Model:** $Z_j \\sim X$ (e.g., Draft Status ~ Performance, Age ~ Performance, etc.)\n",
    "    -   **Assigned to:** Macy\n",
    "\n",
    "3.  **Module 3: `run_dml_crossfit`**\n",
    "    -   **Purpose:** The main \"engine\" that implements the K-fold cross-fitting (Section 6 of our doc). It calls the training functions to generate out-of-sample residuals.\n",
    "    -   **Assigned to:** Tyler + Alberto\n",
    "4.  **Module 4: `train_bias_model`**\n",
    "    -   **Purpose:** The only purpose is to take the out-of-sample residuals and train the final model.\n",
    "    -   **Model:** $\\hat{\\epsilon}_Y \\sim \\hat{\\epsilon}_Z$\n",
    "    -   **Assigned to:** Gary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944bbfe3",
   "metadata": {},
   "source": [
    "## Module 1: Outcome Model ($\\hat{f}$)\n",
    "\n",
    "This function takes training data and returns a *trained* model object that has a `.predict()` method. \n",
    "\n",
    "We need to get the whole pipeline working first, then we can tune or swap models later. Therefore, try to use more simplier models.\n",
    "\n",
    "```python\n",
    "def train_f_model(X_train: pd.DataFrame, y_train: pd.Series) -> Any:\n",
    "    \"\"\"\n",
    "    Trains the outcome model f: Y ~ X (Salary ~ Performance)\n",
    "\n",
    "    Args:\n",
    "        X_train: DataFrame of performance features (training fold).\n",
    "        y_train: Series of the outcome (log_salary) (training fold).\n",
    "\n",
    "    Returns:\n",
    "        A trained model object with a .predict() method.\n",
    "    \"\"\"\n",
    "    print(f\"Training f_model on {X_train.shape[0]} samples...\")\n",
    "\n",
    "    return model_f\n",
    "```\n",
    "\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e65bd8",
   "metadata": {},
   "source": [
    "# Module 2: Treatment Models ($\\hat{h}$)\n",
    "\n",
    "This function is slightly different from Module 1. It takes the training data and returns a *dictionary* of trained models, one for each column in $Z$ (our bias factors). \n",
    "\n",
    "Your task is to train a separate model for each bias factor ($Z_j$) as a function of performance ($X$). The goal is to \"clean\" the bias factors by finding the part of them that cannot be explained by on-court stats.\n",
    "\n",
    "```python\n",
    "def train_h_models(X_train: pd.DataFrame, Z_train: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Trains the treatment models h: Z_j ~ X (Bias Factor ~ Performance)\n",
    "    for each bias factor j in Z.\n",
    "\n",
    "    Args:\n",
    "        X_train: DataFrame of performance features (training fold).\n",
    "        Z_train: DataFrame of contextual/bias factors (training fold).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are Z column names (e.g., 'Draft_Status')\n",
    "        and values are the corresponding trained model objects.\n",
    "    \"\"\"\n",
    "    print(f\"Training h_models for {list(Z_train.columns)}...\")\n",
    "    models_h = {}\n",
    "\n",
    "    return models_h\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ad551",
   "metadata": {},
   "source": [
    "# Module 3: DML Residual Generation Engine\n",
    "\n",
    "This is the main function that orchestrates the DML cross-fitting process. It takes the *full* datasets ($X, Y, Z$) and the *training functions* (Modules 1 & 2) as inputs.\n",
    "\n",
    "This function implements the \"cross-fitting\" algorithm from Section 6 of our project document. Its sole purpose is to generate and return the out-of-sample residuals.\n",
    "\n",
    "Its job is to:\n",
    "\n",
    "1.  Create K-folds.\n",
    "2.  Loop through each fold.\n",
    "3.  On each loop, it calls `model_f_trainer` and `model_h_trainer` on the \"training\" data.\n",
    "4.  It then uses those trained models to generate *predictions* on the \"prediction\" (out-of-sample) data.\n",
    "5.  It calculates and stores the residuals ($\\hat{\\epsilon}_Y$ and $\\hat{\\epsilon}_Z$) for the prediction data.\n",
    "6.  After the loop finishes, it returns the two complete sets of out-of-sample residuals.\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "def generate_dml_residuals(X: pd.DataFrame,\n",
    "                             Y: pd.Series,\n",
    "                             Z: pd.DataFrame,\n",
    "                             model_f_trainer: Callable,\n",
    "                             model_h_trainer: Callable,\n",
    "                             k_folds: int = 5) -> Tuple[pd.Series, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Implements the DML cross-fitting algorithm (Section 6 of the doc).\n",
    "\n",
    "    This function orchestrates the training and prediction across K-folds\n",
    "    to generate out-of-sample residuals for both the outcome (Y) and\n",
    "    the treatment/bias factors (Z).\n",
    "\n",
    "    Args:\n",
    "        X: Full DataFrame of performance features.\n",
    "        Y: Full Series of the outcome (log_salary).\n",
    "        Z: Full DataFrame of contextual/bias factors.\n",
    "        model_f_trainer: The function to train model f (i.e., train_f_model).\n",
    "        model_h_trainer: The function to train models h (i.e., train_h_models).\n",
    "        k_folds: Number of folds for cross-fitting.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - residuals_Y_oos (pd.Series): The out-of-sample residuals for Y (epsilon_Y).\n",
    "        - residuals_Z_oos (pd.DataFrame): The out-of-sample residuals for Z (epsilon_Z).\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting DML Residual Generation...\")\n",
    "\n",
    "    return residuals_Y_oos, residuals_Z_oos\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453491a9",
   "metadata": {},
   "source": [
    "## Module 4: Final OLS Regression Function\n",
    "\n",
    "**Note:** This function implements Step 4 from Section 6 of our doc (\"Run Final OLS\") and follows the \"All-at-Once\" approach from Section 6.1.\n",
    "\n",
    "```python\n",
    "def run_final_ols(residuals_Y: pd.Series, \n",
    "                    residuals_Z: pd.DataFrame) -> RegressionResultsWrapper:\n",
    "    \"\"\"\n",
    "    Runs the final debiased OLS regression on the out-of-sample residuals.\n",
    "\n",
    "    This model estimates:  epsilon_Y ~ epsilon_Z\n",
    "    \n",
    "    This corresponds to Section 6.1 of the methodology document.\n",
    "\n",
    "    Args:\n",
    "        residuals_Y: The out-of-sample residuals for Y (epsilon_Y).\n",
    "        residuals_Z: The out-of-sample residuals for Z (epsilon_Z).\n",
    "\n",
    "    Returns:\n",
    "        The statsmodels OLS results object for the final regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Running final OLS regression on residuals...\")\n",
    "    \n",
    "    return final_ols_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d634074",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
